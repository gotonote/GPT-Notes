# ç¬¬ä¸€ç« ï¼šé¢„è®­ç»ƒè¯¦è§£

> æœ¬ç« å°†è¯¦ç»†ä»‹ç»å¤§è¯­è¨€æ¨¡å‹çš„é¢„è®­ç»ƒé˜¶æ®µï¼ŒåŒ…æ‹¬è®­ç»ƒç›®æ ‡ã€æ•°æ®å¤„ç†ã€è®­ç»ƒæŠ€å·§ç­‰å†…å®¹ã€‚

## 1.1 ä»€ä¹ˆæ˜¯é¢„è®­ç»ƒï¼Ÿ

### 1.1.1 é¢„è®­ç»ƒçš„å®šä¹‰

**é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰** æ˜¯æŒ‡åœ¨ä¸€ä¸ªå¤§è§„æ¨¡é€šç”¨æ•°æ®é›†ä¸Šè®­ç»ƒåŸºç¡€æ¨¡å‹çš„è¿‡ç¨‹ï¼Œä½¿æ¨¡å‹å­¦ä¹ é€šç”¨çš„è¯­è¨€çŸ¥è¯†å’Œè¡¨ç¤ºã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        é¢„è®­ç»ƒé˜¶æ®µ                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   å¤§é‡æ— æ ‡æ³¨æ–‡æœ¬ â”€â”€â–¶ é¢„è®­ç»ƒæ¨¡å‹ â”€â”€â–¶ åŸºç¡€æ¨¡å‹                â”‚
â”‚   (æ•°ç™¾GB~æ•°TB)      (Transformer)    (Foundation Model)   â”‚
â”‚                                                             â”‚
â”‚   ç›®æ ‡ï¼šå­¦ä¹ é€šç”¨è¯­è¨€çŸ¥è¯†å’Œä¸–ç•ŒçŸ¥è¯†                           â”‚
â”‚   ç‰¹ç‚¹ï¼šæ— ç›‘ç£å­¦ä¹ ï¼Œä¸éœ€è¦äººå·¥æ ‡æ³¨                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        å¾®è°ƒé˜¶æ®µ                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   å°‘é‡æ ‡æ³¨æ•°æ® â”€â”€â–¶ åŸºç¡€æ¨¡å‹ â”€â”€â–¶ å¾®è°ƒæ¨¡å‹                    â”‚
â”‚   (æ•°åƒ~æ•°ä¸‡)      (å†»ç»“æƒé‡)    (ä¸“ç”¨æ¨¡å‹)                 â”‚
â”‚                                                             â”‚
â”‚   ç›®æ ‡ï¼šé€‚é…ç‰¹å®šä»»åŠ¡                                        â”‚
â”‚   ç‰¹ç‚¹ï¼šæœ‰ç›‘ç£å­¦ä¹ ï¼Œéœ€è¦äººå·¥æ ‡æ³¨                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.1.2 é¢„è®­ç»ƒ vs å¾®è°ƒ

| ç‰¹æ€§ | é¢„è®­ç»ƒ | å¾®è°ƒ |
|------|--------|------|
| æ•°æ®è§„æ¨¡ | æµ·é‡ï¼ˆTBçº§åˆ«ï¼‰ | å°‘é‡ï¼ˆGBçº§åˆ«ï¼‰ |
| æ•°æ®ç±»å‹ | æ— æ ‡æ³¨æ–‡æœ¬ | æ ‡æ³¨æ•°æ® |
| è®­ç»ƒç›®æ ‡ | è¯­è¨€å»ºæ¨¡ | ä»»åŠ¡ç‰¹å®š |
| è®­ç»ƒæ—¶é—´ | æ•°å‘¨~æ•°æœˆ | æ•°å°æ—¶~æ•°å¤© |
| è®¡ç®—èµ„æº | å¤§é‡GPU | å°‘é‡GPU |

---

## 1.2 é¢„è®­ç»ƒç›®æ ‡ï¼šè¯­è¨€å»ºæ¨¡

### 1.2.1 è‡ªå›å½’è¯­è¨€å»ºæ¨¡

å¤§è¯­è¨€æ¨¡å‹æœ€å¸¸ç”¨çš„é¢„è®­ç»ƒç›®æ ‡æ˜¯ **è‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆAutoregressive Language Modelingï¼‰**ã€‚

ç»™å®šå‰é¢çš„è¯ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼š

```
è¾“å…¥: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æ‰“ç®—"
é¢„æµ‹: "å»"
         â†“
è¾“å…¥: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æ‰“ç®—å»"
é¢„æµ‹: "å…¬å›­"
         â†“
è¾“å…¥: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æ‰“ç®—å»å…¬å›­"
é¢„æµ‹: "æ•£æ­¥"
         â†“
...
è¾“å‡º: "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘æ‰“ç®—å»å…¬å›­æ•£æ­¥"
```

```python
# è‡ªå›å½’è¯­è¨€å»ºæ¨¡ç¤ºä¾‹
import torch
import torch.nn as nn

class AutoregressiveLM(nn.Module):
    def __init__(self, vocab_size, d_model, n_heads, n_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model, n_heads),
            num_layers=n_layers
        )
        self.fc = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        """
        x: (batch_size, seq_len) - token ids
        """
        # åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embedding(x)  # (batch_size, seq_len, d_model)
        
        # Transformer ç¼–ç 
        x = self.transformer(x)  # (batch_size, seq_len, d_model)
        
        # é¢„æµ‹ä¸‹ä¸€ä¸ªè¯
        logits = self.fc(x)  # (batch_size, seq_len, vocab_size)
        
        # è¿”å›æ—¶éœ€è¦å°†ç›®æ ‡å‘åç§»åŠ¨ä¸€ä½
        return logits[:, :-1, :], x[:, 1:, :]

# æŸå¤±è®¡ç®—
def compute_loss(logits, targets):
    """
    logits: (batch_size, seq_len-1, vocab_size)
    targets: (batch_size, seq_len-1)
    """
    loss_fn = nn.CrossEntropyLoss()
    # è°ƒæ•´å½¢çŠ¶ä»¥è®¡ç®—äº¤å‰ç†µ
    loss = loss_fn(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))
    return loss
```

### 1.2.2 Masked Language Modeling (MLM)

BERT ç­‰æ¨¡å‹ä½¿ç”¨ **Masked Language Modeling** ä½œä¸ºé¢„è®­ç»ƒç›®æ ‡ã€‚

```
åŸå§‹å¥å­: "ä»Šå¤©å¤©æ°”å¾ˆå¥½"

éšæœº Mask: "ä»Šå¤©[MASK]å¾ˆå¥½"
          "ä»Šå¤©å¤©æ°”[MASK]" 
          "[MASK]å¤©æ°”å¾ˆå¥½"

ç›®æ ‡: é¢„æµ‹è¢« Mask ä½ç½®çš„è¯
```

```python
# MLM è®­ç»ƒç¤ºä¾‹
class MLMTrainer:
    def __init__(self, model, tokenizer, mask_token_id):
        self.model = model
        self.tokenizer = tokenizer
        self.mask_token_id = mask_token_id
    
    def prepare_mlm_data(self, text, mask_prob=0.15):
        """
        å‡†å¤‡ MLM è®­ç»ƒæ•°æ®
        """
        # åˆ†è¯
        tokens = self.tokenizer.tokenize(text)
        labels = tokens.copy()
        
        # éšæœº mask
        for i in range(len(tokens)):
            if i == 0 or i == len(tokens) - 1:  # ä¿ç•™ CLS å’Œ SEP
                continue
            if random.random() < mask_prob:
                if random.random() < 0.8:
                    tokens[i] = '[MASK]'
                elif random.random() < 0.5:
                    # éšæœºæ›¿æ¢ä¸ºå…¶ä»–è¯
                    tokens[i] = random.choice(tokens)
                # 10% ä¿æŒä¸å˜ï¼Œä½†ä»æœ‰æ ‡ç­¾
        
        return tokens, labels
```

---

## 1.3 è®­ç»ƒæ•°æ®

### 1.3.1 æ•°æ®æ¥æº

å¤§æ¨¡å‹çš„é¢„è®­ç»ƒæ•°æ®æ¥æºéå¸¸å¹¿æ³›ï¼š

| æ•°æ®ç±»å‹ | æ¥æº | æ¯”ä¾‹ |
|----------|------|------|
| ç½‘é¡µæ–‡æœ¬ | Common Crawlã€CommonSpider | ~60-80% |
| ä¹¦ç± | Project Gutenbergã€BookCorpus | ~10-20% |
| ä»£ç  | GitHubã€Stack Overflow | ~5-10% |
| è®ºæ–‡ | arXivã€PubMed | ~5% |
| ç™¾ç§‘ | Wikipedia | ~3% |
| å¯¹è¯ | Redditã€ç¤¾äº¤åª’ä½“ | ~5% |

### 1.3.2 æ•°æ®é¢„å¤„ç†

```python
# å…¸å‹æ•°æ®é¢„å¤„ç†æµç¨‹
import re
import html

def preprocess_text(text):
    """
    æ–‡æœ¬é¢„å¤„ç†
    """
    # 1. HTML æ ‡ç­¾æ¸…ç†
    text = html.unescape(text)
    text = re.sub(r'<[^>]+>', '', text)
    
    # 2. URL æ›¿æ¢
    text = re.sub(r'http[s]?://\S+', '[URL]', text)
    
    # 3. é‚®ç®±æ›¿æ¢
    text = re.sub(r'\S+@\S+', '[EMAIL]', text)
    
    # 4. å¤šä½™ç©ºç™½æ¸…ç†
    text = re.sub(r'\s+', ' ', text)
    
    # 5. ç»Ÿä¸€æ¢è¡Œç¬¦
    text = text.replace('\r\n', '\n')
    
    # 6. å»é™¤è¿‡çŸ­æˆ–è¿‡é•¿çš„æ–‡æœ¬
    if len(text) < 100 or len(text) > 100000:
        return None
    
    return text.strip()

def dedup_texts(texts):
    """
    æ–‡æœ¬å»é‡
    """
    seen = set()
    unique_texts = []
    
    for text in texts:
        # ä½¿ç”¨ hash å»é‡
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash not in seen:
            seen.add(text_hash)
            unique_texts.append(text)
    
    return unique_texts
```

### 1.3.3 æ•°æ®è´¨é‡è¿‡æ»¤

```python
# æ•°æ®è´¨é‡è¿‡æ»¤ç¤ºä¾‹
def quality_filter(text, min_length=100):
    """
    è´¨é‡è¿‡æ»¤
    """
    # 1. é•¿åº¦æ£€æŸ¥
    if len(text) < min_length:
        return False
    
    # 2. å­—ç¬¦æ¯”ä¾‹æ£€æŸ¥
    # è¿‡æ»¤æ‰ç‰¹æ®Šå­—ç¬¦è¿‡å¤šçš„æ–‡æœ¬
    special_chars = sum(1 for c in text if not c.isalnum() and c not in ' \n.,!?;:\'"')
    if special_chars / len(text) > 0.5:
        return False
    
    # 3. é‡å¤ç‡æ£€æŸ¥
    lines = text.split('\n')
    unique_lines = set(lines)
    if len(unique_lines) / len(lines) < 0.3:
        return False
    
    # 4. è¯­è¨€æ£€æµ‹ï¼ˆå¯é€‰ï¼‰
    # ä½¿ç”¨ langdetect è¿‡æ»¤éç›®æ ‡è¯­è¨€
    
    return True
```

---

## 1.4 è®­ç»ƒæŠ€å·§

### 1.4.1 åˆ†å¸ƒå¼è®­ç»ƒ

å¤§æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºï¼Œé€šå¸¸ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒã€‚

```python
# ä½¿ç”¨ DeepSpeed è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ
from deepspeed import DeepSpeedConfig
import deepspeed

# DeepSpeed é…ç½®
ds_config = {
    "train_batch_size": 32,
    "gradient_accumulation_steps": 4,
    "fp16": {"enabled": True},
    "zero_optimization": {
        "stage": 3,
        "offload_param": {"device": "cpu"},
        "offload_optimizer": {"device": "cpu"}
    },
    "steps_per_print": 100
}

# åˆå§‹åŒ–æ¨¡å‹
model = YourModel()

# DeepSpeed è®­ç»ƒ
model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    optimizer=optimizer,
    config=ds_config
)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    loss = model_engine(batch)
    model_engine.backward(loss)
    model_engine.step()
```

### 1.4.2 æ··åˆç²¾åº¦è®­ç»ƒ

```python
# PyTorch æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    # å‰å‘ä¼ æ’­ - è‡ªåŠ¨è½¬æ¢ä¸º FP16
    with autocast():
        outputs = model(batch)
        loss = criterion(outputs, targets)
    
    # åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    
    # æ¢¯åº¦è£å‰ª
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    # å‚æ•°æ›´æ–°
    scaler.step(optimizer)
    scaler.update()
```

### 1.4.3 æ¢¯åº¦ç´¯ç§¯

å½“ GPU æ˜¾å­˜ä¸è¶³æ—¶ï¼Œå¯ä»¥ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š

```python
# æ¢¯åº¦ç´¯ç§¯ç¤ºä¾‹
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps

optimizer.zero_grad()

for i, batch in enumerate(dataloader):
    loss = model(batch)
    loss = loss / accumulation_steps  # å½’ä¸€åŒ–æŸå¤±
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

---

## 1.5 è®­ç»ƒåŸºç¡€è®¾æ–½

### 1.5.1 ç¡¬ä»¶é…ç½®

å…¸å‹çš„å¤§æ¨¡å‹é¢„è®­ç»ƒé›†ç¾¤é…ç½®ï¼š

| æ¨¡å‹è§„æ¨¡ | GPU æ•°é‡ | GPU ç±»å‹ | è®­ç»ƒæ—¶é—´ |
|----------|----------|----------|----------|
| 7B | 8-16 | A100 80GB | ~1-2 å‘¨ |
| 70B | 64-128 | A100 80GB | ~1-2 æœˆ |
| 175B | 1000+ | A100 80GB | ~æ•°æœˆ |

### 1.5.2 è®­ç»ƒæ¡†æ¶

```python
# ä½¿ç”¨ Megatron-LM è¿›è¡Œè®­ç»ƒ
from megatron import get_args, initialize_model_parallel
from megatron.model import GPTModel

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
initialize_model_pipeline(...)
initialize_model_tensor_model_parallel(...)

# åˆ›å»ºæ¨¡å‹
model = GPTModel(
    num_tokentypes=0,
    parallel_output=True,
    pre_process=get_args().pre_process,
    post_process=get_args().post_process
)
```

---

## 1.6 é¢„è®­ç»ƒæ¨¡å‹ç¤ºä¾‹

### 1.6.1 LLaMA é¢„è®­ç»ƒé…ç½®

```python
# LLaMA é¢„è®­ç»ƒé…ç½®ç¤ºä¾‹
llama_config = {
    "vocab_size": 32000,
    "hidden_size": 4096,
    "intermediate_size": 11008,
    "num_hidden_layers": 32,
    "num_attention_heads": 32,
    "num_key_value_heads": 32,  # Grouped Query Attention
    "max_position_embeddings": 2048,
    "rms_norm_eps": 1e-6,
    "rope_theta": 10000.0,
    "attention_bias": False,
    "torch_dtype": "bfloat16",
}
```

### 1.6.2 è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | å…¸å‹å€¼ | è¯´æ˜ |
|------|--------|------|
| å­¦ä¹ ç‡ | 1e-4 ~ 3e-4 | è¾ƒå¤§æ¨¡å‹ç”¨è¾ƒå°å­¦ä¹ ç‡ |
| æ‰¹å¤§å° | 1M+ tokens | æœ‰æ•ˆæ‰¹å¤§å° |
| é¢„çƒ­æ­¥æ•° | 1000-2000 | å­¦ä¹ ç‡é¢„çƒ­ |
| æƒé‡è¡°å‡ | 0.1 | AdamW æƒé‡è¡°å‡ |
| æ¢¯åº¦è£å‰ª | 1.0 | é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ |
| è®­ç»ƒæ­¥æ•° | 100K-1M | æ ¹æ®æ•°æ®é‡è°ƒæ•´ |

---

## 1.7 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **é¢„è®­ç»ƒçš„å®šä¹‰**ï¼šåœ¨å¤§è§„æ¨¡æ•°æ®ä¸Šå­¦ä¹ é€šç”¨è¯­è¨€çŸ¥è¯†
2. **è®­ç»ƒç›®æ ‡**ï¼šè‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼ˆGPT ç³»åˆ—ï¼‰å’Œ MLMï¼ˆBERT ç³»åˆ—ï¼‰
3. **æ•°æ®å¤„ç†**ï¼šæ•°æ®æ¥æºã€é¢„å¤„ç†ã€è´¨é‡è¿‡æ»¤
4. **è®­ç»ƒæŠ€å·§**ï¼šåˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦ã€æ¢¯åº¦ç´¯ç§¯
5. **åŸºç¡€è®¾æ–½**ï¼šç¡¬ä»¶é…ç½®ã€è®­ç»ƒæ¡†æ¶

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
- [DeepSpeed: System Optimizations for Deep Learning](https://www.microsoft.com/en-us/research/blog/deepspeed-learning)

---

*ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ å¾®è°ƒæŠ€æœ¯ï¼Œäº†è§£å¦‚ä½•å°†é¢„è®­ç»ƒæ¨¡å‹é€‚é…åˆ°ç‰¹å®šä»»åŠ¡ã€‚*
