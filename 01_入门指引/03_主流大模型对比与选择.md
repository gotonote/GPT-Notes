# ç¬¬ä¸‰ç« ï¼šä¸»æµå¤§æ¨¡å‹å¯¹æ¯”ä¸é€‰æ‹©

> æœ¬ç« å°†ä»‹ç»å½“å‰ä¸»æµçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œå¸®åŠ©ä½ äº†è§£ä¸åŒæ¨¡å‹çš„ç‰¹ç‚¹ï¼Œå¹¶æ ¹æ®å®é™…éœ€æ±‚åšå‡ºé€‰æ‹©ã€‚

## 3.1 å¤§æ¨¡å‹åˆ†ç±»

### 3.1.1 æŒ‰å¼€å‘è€…åˆ†ç±»

| ç±»åˆ« | ä»£è¡¨æ¨¡å‹ | ç‰¹ç‚¹ |
|------|----------|------|
| **OpenAI ç³»åˆ—** | GPT-4, GPT-3.5 | èƒ½åŠ›æœ€å¼ºé—­æºï¼ŒAPI ç”Ÿæ€å®Œå–„ |
| **Meta å¼€æºç³»åˆ—** | LLaMA 3, LLaMA 2 | å¼€æºå¯å•†ç”¨ï¼Œæ€§èƒ½å‡ºè‰² |
| **Google ç³»åˆ—** | Gemini, PaLM | å¤šæ¨¡æ€èƒ½åŠ›å¼º |
| **Anthropic ç³»åˆ—** | Claude 3 | å®‰å…¨å¯¹é½å¥½ï¼Œé•¿æ–‡æœ¬ |
| **ä¸­å›½å‚å•†** | Qwen, ChatGLM, Kimi | ä¸­æ–‡ä¼˜åŒ–ï¼Œå¼€æºå‹å¥½ |

### 3.1.2 æŒ‰åŠŸèƒ½åˆ†ç±»

```
å¤§è¯­è¨€æ¨¡å‹
â”œâ”€â”€ é€šç”¨å¤§æ¨¡å‹
â”‚   â”œâ”€â”€ GPT-4 / GPT-4o
â”‚   â”œâ”€â”€ Claude 3
â”‚   â”œâ”€â”€ Gemini
â”‚   â””â”€â”€ LLaMA 3
â”‚
â”œâ”€â”€ ç¼–ç¨‹æ¨¡å‹
â”‚   â”œâ”€â”€ CodeLlama
â”‚   â”œâ”€â”€ DeepSeek Coder
â”‚   â””â”€â”€ é€šä¹‰çµç 
â”‚
â”œâ”€â”€ å¤šæ¨¡æ€æ¨¡å‹
â”‚   â”œâ”€â”€ GPT-4V
â”‚   â”œâ”€â”€ Claude 3 (Vision)
â”‚   â”œâ”€â”€ Gemini Vision
â”‚   â””â”€â”€ LLaVA
â”‚
â””â”€â”€ å‚ç›´é¢†åŸŸæ¨¡å‹
    â”œâ”€â”€ åŒ»ç–—ï¼šMed-PaLM
    â”œâ”€â”€ æ³•å¾‹ï¼šLegalBERT
    â””â”€â”€ é‡‘èï¼šBloombergGPT
```

---

## 3.2 ä¸»æµæ¨¡å‹è¯¦ç»†å¯¹æ¯”

### 3.2.1 GPT ç³»åˆ—

| æ¨¡å‹ | å‘å¸ƒæ—¶é—´ | ä¸Šä¸‹æ–‡ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|----------|--------|------|----------|
| **GPT-4o** | 2024.05 | 128K | æœ€æ–°å¤šæ¨¡æ€æ——èˆ° | é«˜ç«¯ä»»åŠ¡ |
| **GPT-4 Turbo** | 2023.11 | 128K | æ€§ä»·æ¯”é«˜ | æ—¥å¸¸å¼€å‘ |
| **GPT-3.5 Turbo** | 2022.11 | 16K | ä¾¿å®œå¿«é€Ÿ | ç®€å•ä»»åŠ¡ |

```python
# OpenAI API é€‰æ‹©å»ºè®®
if task_complexity == "high":
    model = "gpt-4o"  # å¤æ‚æ¨ç†ã€ç¼–ç¨‹
elif task_complexity == "medium":
    model = "gpt-4-turbo"  # å¯¹è¯ã€å†™ä½œ
else:
    model = "gpt-3.5-turbo"  # ç®€å•é—®ç­”ã€å®¢æœ
```

### 3.2.2 Claude ç³»åˆ—

| æ¨¡å‹ | å‘å¸ƒæ—¶é—´ | ä¸Šä¸‹æ–‡ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|----------|--------|------|----------|
| **Claude 3.5 Sonnet** | 2024.06 | 200K | æ€§ä»·æ¯”æœ€ä½³ | é€šç”¨ä»»åŠ¡ |
| **Claude 3 Opus** | 2024.03 | 200K | æœ€å¼ºèƒ½åŠ› | å¤æ‚æ¨ç† |
| **Claude 3 Haiku** | 2024.03 | 200K | å¿«é€Ÿä¾¿å®œ | ç®€å•ä»»åŠ¡ |

> ğŸ’¡ **æç¤º**ï¼šClaude åœ¨é•¿æ–‡æœ¬å¤„ç†å’Œä»£ç èƒ½åŠ›æ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œç‰¹åˆ«é€‚åˆå¤„ç†é•¿æ–‡æ¡£ã€‚

### 3.2.3 å¼€æºæ¨¡å‹

| æ¨¡å‹ | å‚æ•° | ç‰¹ç‚¹ | éƒ¨ç½²éš¾åº¦ |
|------|------|------|----------|
| **LLaMA 3 70B** | 70B | å¼€æºå¯å•†ç”¨ï¼Œæ€§èƒ½æ¥è¿‘ GPT-4 | éœ€è¦ GPU |
| **LLaMA 3 8B** | 8B | æ¶ˆè´¹çº§æ˜¾å¡å¯è¿è¡Œ | ä¸­ç­‰ |
| **Qwen 2 72B** | 72B | ä¸­æ–‡èƒ½åŠ›æœ€å¼º | éœ€è¦ GPU |
| **Qwen 2 7B** | 7B | æ¶ˆè´¹çº§å¯è¿è¡Œ | ç®€å• |
| **ChatGLM 4** | 6B/130B | ä¸­æ–‡å¯¹è¯å‹å¥½ | ä¸­ç­‰/éš¾ |

```bash
# æœ¬åœ°éƒ¨ç½² LLaMA 3 8B æ¨èé…ç½®
# æœ€ä½ï¼š24GB æ˜¾å­˜ (RTX 3090/4090)
# æ¨èï¼š32GB+ æ˜¾å­˜

# ä½¿ç”¨ transformers åŠ è½½
from transformers import AutoTokenizer, AutoModelForCausalLM

model = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model)
model = AutoModelForCausalLM.from_pretrained(
    model, 
    device_map="auto",
    load_in_8bit=True  # 8ä½é‡åŒ–
)
```

---

## 3.3 å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹

### 3.3.1 é€‰æ‹©å†³ç­–æ ‘

```
éœ€è¦é€‰æ‹©æ¨¡å‹?
â”‚
â”œâ”€ æ˜¯å¦éœ€è¦æœ¬åœ°éƒ¨ç½²?
â”‚   â”‚
â”‚   â”œâ”€ æ˜¯ â†’ è€ƒè™‘ç¡¬ä»¶æ¡ä»¶
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€ 8GB+ VRAM â†’ Qwen 2 7B / LLaMA 3 8B
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€ 24GB+ VRAM â†’ LLaMA 3 70B / Qwen 2 72B
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€ 80GB+ A100 â†’ å®Œæ•´ç²¾åº¦æ¨¡å‹
â”‚   â”‚
â”‚   â””â”€ å¦ â†’ ä½¿ç”¨ API æœåŠ¡
â”‚
â”œâ”€ ä¸»è¦ä½¿ç”¨è¯­è¨€?
â”‚   â”‚
â”‚   â”œâ”€ ä¸­æ–‡ â†’ Qwen 2 / ChatGLM / æ™ºè°±æ¸…è¨€
â”‚   â”‚
â”‚   â””â”€ è‹±æ–‡ â†’ GPT-4 / Claude 3 / LLaMA 3
â”‚
â”œâ”€ æ˜¯å¦éœ€è¦å¤šæ¨¡æ€?
â”‚   â”‚
â”‚   â”œâ”€ æ˜¯ â†’ GPT-4V / Claude 3 Vision / Gemini
â”‚   â”‚
â”‚   â””â”€ å¦ â†’ çº¯æ–‡æœ¬æ¨¡å‹å³å¯
â”‚
â””â”€ é¢„ç®—è€ƒè™‘?
    â”‚
    â”œâ”€ å…è´¹/ä½æˆæœ¬ â†’ å¼€æºæ¨¡å‹ + Ollama
    â”‚
    â”œâ”€ ä½é¢„ç®— â†’ GPT-3.5 / Claude Haiku
    â”‚
    â””â”€ é«˜é¢„ç®— â†’ GPT-4 / Claude Opus
```

### 3.3.2 åœºæ™¯æ¨è

| åœºæ™¯ | æ¨èæ¨¡å‹ | ç†ç”± |
|------|----------|------|
| **æ—¥å¸¸å¯¹è¯** | GPT-3.5 / Claude Haiku | ä¾¿å®œã€å¿«é€Ÿ |
| **ä»£ç å¼€å‘** | GPT-4 / Claude 3 Opus | ç¼–ç¨‹èƒ½åŠ›å¼º |
| **é•¿æ–‡æ¡£å¤„ç†** | Claude 3 (200K ä¸Šä¸‹æ–‡) | é•¿æ–‡æœ¬ä¼˜åŠ¿ |
| **ä¸­æ–‡å†…å®¹åˆ›ä½œ** | Qwen 2 / ChatGLM | ä¸­æ–‡ä¼˜åŒ– |
| **æœ¬åœ°éƒ¨ç½²å­¦ä¹ ** | LLaMA 3 8B / Qwen 2 7B | æ¶ˆè´¹çº§å¯è·‘ |
| **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²** | LLaMA 3 70B / Qwen 2 72B | æ€§èƒ½ä¸æˆæœ¬å¹³è¡¡ |

### 3.3.3 æˆæœ¬å¯¹æ¯”

| æ¨¡å‹ | è¾“å…¥ä»·æ ¼ | è¾“å‡ºä»·æ ¼ | 1000æ¬¡è°ƒç”¨(å¹³å‡) |
|------|----------|----------|------------------|
| GPT-4o | $5/1M | $15/1M | ~$10 |
| GPT-4 Turbo | $10/1M | $30/1M | ~$20 |
| GPT-3.5 Turbo | $0.5/1M | $1.5/1M | ~$1 |
| Claude 3.5 Sonnet | $3/1M | $15/1M | ~$9 |
| Claude 3 Haiku | $0.25/1M | $1.25/1M | ~$0.75 |

> ğŸ’¡ **ä¼˜åŒ–å»ºè®®**ï¼šå¯¹äºç®€å•ä»»åŠ¡ä½¿ç”¨å°æ¨¡å‹ï¼Œå¯ä»¥å¤§å¹…é™ä½æˆæœ¬ã€‚

---

## 3.4 API å¯¹æ¯”ä¸ä½¿ç”¨

### 3.4.1 ä¸»æµ API å¯¹æ¯”

| æä¾›å•† | API æ ¼å¼ | SDK æ”¯æŒ | å›½å†…è®¿é—® |
|--------|----------|----------|----------|
| **OpenAI** | OpenAI API | Python, Node.js | éœ€ä»£ç† |
| **Anthropic** | Claude API | Python, Node.js | éœ€ä»£ç† |
| **é˜¿é‡Œäº‘** | DashScope | Python, Java | ç›´æ¥è®¿é—® |
| **æ™ºè°±AI** | GLM API | Python, Node.js | ç›´æ¥è®¿é—® |

### 3.4.2 OpenAI API ä½¿ç”¨

```python
import openai

# åŸºç¡€è°ƒç”¨
response = openai.ChatCompletion.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šåŠ©æ‰‹"},
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}
    ],
    temperature=0.7,
    max_tokens=500
)

# è·å–å›å¤
print(response.choices[0].message.content)
```

### 3.4.3 Claude API ä½¿ç”¨

```python
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=500,
    messages=[
        {"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "}
    ]
)

print(response.content[0].text)
```

### 3.4.4 å›½å†… API ä½¿ç”¨

```python
# é˜¿é‡Œäº‘ DashScope (Qwen)
from dashscope import Generation

response = Generation.call(
    model="qwen-turbo",
    prompt="è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ "
)

print(response.output.text)
```

---

## 3.5 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†ï¼š

1. **å¤§æ¨¡å‹çš„åˆ†ç±»**ï¼šæŒ‰å¼€å‘è€…ã€æŒ‰åŠŸèƒ½ã€æŒ‰ç”¨é€”
2. **ä¸»æµæ¨¡å‹å¯¹æ¯”**ï¼šGPTã€Claudeã€å¼€æºæ¨¡å‹çš„ç‰¹ç‚¹
3. **æ¨¡å‹é€‰æ‹©ç­–ç•¥**ï¼šæ ¹æ®åœºæ™¯ã€é¢„ç®—ã€è¯­è¨€é€‰æ‹©åˆé€‚çš„æ¨¡å‹
4. **API ä½¿ç”¨**ï¼šä¸»æµ API çš„è°ƒç”¨æ–¹å¼

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [OpenAI æ¨¡å‹å®šä»·](https://openai.com/pricing)
- [Anthropic Claude å®šä»·](https://www.anthropic.com/pricing)
- [LLaMA 3 å®˜æ–¹é¡µé¢](https://ai.meta.com/llama/)
- [Qwen 2 å¼€æºé¡µé¢](https://github.com/QwenLM/Qwen2)

---

*ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹ å¤§æ¨¡å‹çš„åŸºç¡€æ¦‚å¿µï¼Œæ·±å…¥ç†è§£ Tokenã€Embedding ç­‰æ ¸å¿ƒæœ¯è¯­ã€‚*
