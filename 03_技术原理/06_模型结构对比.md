# 06 æ¨¡å‹ç»“æ„å¯¹æ¯”

> GPT vs BERT vs T5ï¼šç†è§£ä¸åŒæ¶æ„çš„ä¼˜åŠ£

## ğŸ“Š ä¸»æµæ¨¡å‹æ¶æ„

| æ¶æ„ | ç‰¹ç‚¹ | è®­ç»ƒç›®æ ‡ | ä»£è¡¨æ¨¡å‹ |
|------|------|----------|----------|
| **Encoder-only** | åŒå‘ç†è§£ | MLM | BERT |
| **Decoder-only** | è‡ªå›å½’ç”Ÿæˆ | LM | GPT |
| **Encoder-Decoder** | åºåˆ—åˆ°åºåˆ— | Seq2Seq | T5, BART |

---

## ğŸ”¬ è¯¦ç»†å¯¹æ¯”

### 1. Encoder-onlyï¼ˆBERT ç³»åˆ—ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåŒå‘æ³¨æ„åŠ›ï¼Œç†è§£æ•´ä¸ªåºåˆ—

```
è¾“å…¥: "ä»Šå¤©[mask]å¤©æ°”å¾ˆå¥½"
è¾“å‡º: é¢„æµ‹ [mask] = "çš„"
```

```python
# BERT å‰å‘ä¼ æ’­
from transformers import BertModel, BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

text = "ä»Šå¤©[MASK]å¤©æ°”å¾ˆå¥½"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# è·å– [MASK] ä½ç½®çš„é¢„æµ‹
mask_token_id = tokenizer.mask_token_id
mask_position = inputs['input_ids'][0].tolist().index(mask_token_id)
logits = outputs.logits[0, mask_position]

# é¢„æµ‹
predicted_id = logits.argmax().item()
predicted_token = tokenizer.decode(predicted_id)
print(f"é¢„æµ‹: {predicted_token}")  # è¾“å‡º: çš„
```

**ç‰¹ç‚¹**ï¼š
- âœ“ é€‚åˆç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERï¼‰
- âœ— ä¸é€‚åˆç”Ÿæˆä»»åŠ¡
- âœ“ åŒå‘æ³¨æ„åŠ›ï¼Œä¿¡æ¯æµåŠ¨å¥½

### 2. Decoder-onlyï¼ˆGPT ç³»åˆ—ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šå•å‘æ³¨æ„åŠ›ï¼Œè‡ªå›å½’ç”Ÿæˆ

```python
# GPT è‡ªå›å½’ç”Ÿæˆ
from transformers import GPT2LMHeadModel, GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

prompt = "The future of AI is"
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_length=50,
    temperature=0.7,
    do_sample=True
)

result = tokenizer.decode(outputs[0])
print(result)
```

**ç‰¹ç‚¹**ï¼š
- âœ“ é€‚åˆç”Ÿæˆä»»åŠ¡
- âœ— åªèƒ½çœ‹åˆ°ä¹‹å‰çš„å†…å®¹
- âœ“ æ˜“äºæ‰©å±•ï¼ˆScalingï¼‰

### 3. Encoder-Decoderï¼ˆT5 ç³»åˆ—ï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šç¼–ç ç†è§£ + è§£ç ç”Ÿæˆ

```
è¾“å…¥: "ç¿»è¯‘æˆè‹±æ–‡: æˆ‘çˆ±ä½ "
è¾“å‡º: "I love you"
```

```python
# T5 ç¿»è¯‘ç¤ºä¾‹
from transformers import T5Tokenizer, T5ForConditionalGeneration

tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# ç¿»è¯‘ä»»åŠ¡
input_text = "translate English to German: I love you"
inputs = tokenizer(input_text, return_tensors="pt")

outputs = model.generate(**inputs)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)  # è¾“å‡º: Ich liebe dich
```

**ç‰¹ç‚¹**ï¼š
- âœ“ é€‚åˆåºåˆ—åˆ°åºåˆ—ä»»åŠ¡
- âœ“ ç¼–ç å™¨-è§£ç å™¨è§£è€¦
- âœ— å‚æ•°é‡å¤§

---

## ğŸ—ï¸ æ¶æ„å¯è§†åŒ–

```
Encoder-only (BERT):
[CLS] ä»Š å¤© å¤© æ°” [SEP]
  â†‘   â†‘   â†‘   â†‘   â†‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  åŒå‘æ³¨æ„åŠ›

Decoder-only (GPT):
ä»Š å¤© å¤© æ°” çœŸ å¥½
â†‘ â†‘ â†‘ â†‘ â†‘ â†‘
â”‚ â”‚ â”‚ â”‚ â”‚ â””â”€â”€ çœ‹åˆ°æ‰€æœ‰å‰é¢çš„
â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€â”€
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Encoder-Decoder (T5):
ç¼–ç å™¨: ä»Š å¤© å¤© æ°” â†’ [H] [H] [H] [H]
                      â†“
è§£ç å™¨: â†’ çœŸ â†’ å¥½ â†’ [EOS]
         â†‘    â†‘    â†‘
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Cross Attention
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| ä»»åŠ¡ | Encoder-only | Decoder-only | Encoder-Decoder |
|------|--------------|---------------|-----------------|
| æ–‡æœ¬åˆ†ç±» | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | â˜…â˜…â˜…â˜… |
| å‘½åå®ä½“è¯†åˆ« | â˜…â˜…â˜…â˜…â˜… | â˜…â˜… | â˜…â˜…â˜…â˜… |
| æœºå™¨ç¿»è¯‘ | â˜…â˜… | â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| æ–‡æœ¬ç”Ÿæˆ | â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… |
| é—®ç­”ä»»åŠ¡ | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |

---

## ğŸ§® å‚æ•°é‡ä¼°ç®—

### Transformer å±‚å‚æ•°

```
æ¯å±‚å‚æ•°é‡ â‰ˆ 4 Ã— d_modelÂ² + 4 Ã— d_model Ã— d_ff + 2 Ã— d_modelÂ²

å…¶ä¸­:
- 4 Ã— d_modelÂ²: Q, K, V, O çŸ©é˜µ
- 4 Ã— d_model Ã— d_ff: FFN ä¸­ä¸¤ä¸ªçº¿æ€§å±‚
- 2 Ã— d_modelÂ²: å±‚å½’ä¸€åŒ–ï¼ˆå¯å¿½ç•¥ï¼‰
```

### BERT-base vs GPT-2 vs T5-small

| æ¨¡å‹ | å±‚æ•° | éšè—ç»´åº¦ | FFN ç»´åº¦ | æ³¨æ„åŠ›å¤´ | å‚æ•°é‡ |
|------|------|----------|----------|----------|--------|
| BERT-base | 12 | 768 | 3072 | 12 | 110M |
| GPT-2 | 48 | 1024 | 4096 | 16 | 1.5B |
| T5-small | 6 | 512 | 2048 | 8 | 60M |

---

## ğŸ’¡ æ¶æ„é€‰æ‹©æŒ‡å—

```
éœ€è¦æ ¹æ®ä»»åŠ¡é€‰æ‹©ï¼š

1. æ–‡æœ¬åˆ†ç±»/NER â†’ Encoder-only (BERT)
2. æ–‡æœ¬ç”Ÿæˆ â†’ Decoder-only (GPT)
3. ç¿»è¯‘/æ‘˜è¦ â†’ Encoder-Decoder (T5/BART)
4. å¯¹è¯ç”Ÿæˆ â†’ Decoder-only (GPT)
5. çŸ¥è¯†å¯†é›†å‹ â†’ Decoder + RAG
```

---

## ğŸ”„ æ··åˆæ¶æ„

### Encoder-Decoder â†’ Decoder-only

Flash Attention å‡ºç°åï¼ŒEncoder-Decoder é€æ¸è¢« Decoder-only å–ä»£ï¼š

```
T5 (Encoder-Decoder) â†’ é€æ¸å°‘ç”¨
LLaMA (Decoder-only) â†’ ä¸»æµ
```

### åŸå› ï¼š
1. **è®­ç»ƒæ•ˆç‡**ï¼šDecoder-only æ›´é€‚åˆå¤§è§„æ¨¡è®­ç»ƒ
2. **æ¨ç†æ•ˆç‡**ï¼šå¯ä»¥ç”¨ KV Cache åŠ é€Ÿ
3. **æ¶Œç°èƒ½åŠ›**ï¼šå¤§å‚æ•°é‡çš„ Decoder è¡¨ç°æ›´å¥½

---

## ğŸ¯ å°ç»“

1. **Encoder-only**ï¼šé€‚åˆç†è§£ä»»åŠ¡ï¼ŒBERT ç³»åˆ—
2. **Decoder-only**ï¼šé€‚åˆç”Ÿæˆä»»åŠ¡ï¼ŒGPT ç³»åˆ—
3. **Encoder-Decoder**ï¼šé€‚åˆ Seq2Seqï¼ŒT5 ç³»åˆ—
4. å½“å‰è¶‹åŠ¿ï¼š**Decoder-only** ä¸»å¯¼
5. é€‰æ‹©ä¾æ®ï¼š**ä»»åŠ¡éœ€æ±‚** è€Œéæ¨¡å‹çƒ­åº¦

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
- [Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683)

---

*ğŸ”œ ä¸‹ä¸€ç« ï¼š[07_è®­ç»ƒç›®æ ‡](./07_è®­ç»ƒç›®æ ‡.md)*
