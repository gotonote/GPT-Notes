# 01 Transformerå…¥é—¨

> ç†è§£é©å‘½æ€§çš„æ¶æ„è®¾è®¡ï¼Œå¼€å¯å¤§è¯­è¨€æ¨¡å‹ä¹‹æ—…

## ğŸ“– ä»€ä¹ˆæ˜¯ Transformerï¼Ÿ

**Transformer** æ˜¯ 2017 å¹´ç”± Google åœ¨è®ºæ–‡ã€ŠAttention Is All You Needã€‹ä¸­æå‡ºçš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œå®ƒå®Œå…¨åŸºäº**æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttention Mechanismï¼‰**ï¼Œæ‘’å¼ƒäº†ä¼ ç»Ÿçš„ RNN å’Œ CNN ç»“æ„ã€‚

### ğŸŒŸ æ ¸å¿ƒç‰¹ç‚¹

| ç‰¹ç‚¹ | è¯´æ˜ |
|------|------|
| **å¹¶è¡Œè®¡ç®—** | æ”¯æŒ GPU å¹¶è¡Œå¤„ç†ï¼Œå¤§å¹…æå‡è®­ç»ƒæ•ˆç‡ |
| **é•¿è·ç¦»ä¾èµ–** | æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥å»ºæ¨¡ä»»æ„ä½ç½®çš„å…³ç³» |
| **å¯æ‰©å±•æ€§** | æ¨¡å‹è§„æ¨¡å¯ä»¥ scaling up |
| **é€šç”¨æ€§å¼º** | é€‚ç”¨äº NLPã€CVã€è¯­éŸ³ç­‰å¤šç§ä»»åŠ¡ |

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

Transformer é‡‡ç”¨ **ç¼–ç å™¨-è§£ç å™¨ï¼ˆEncoder-Decoderï¼‰** ç»“æ„ï¼š

```
è¾“å…¥åºåˆ— â†’ [ç¼–ç å™¨] â†’ ä¸Šä¸‹æ–‡è¡¨ç¤º â†’ [è§£ç å™¨] â†’ è¾“å‡ºåºåˆ—
```

### ç¼–ç å™¨ï¼ˆEncoderï¼‰

- ç”± N ä¸ªç›¸åŒçš„ **Transformer Block** ç»„æˆ
- æ¯ä¸ª Block åŒ…å«ï¼š
  - å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆMulti-Head Self-Attentionï¼‰
  - å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed Forward Networkï¼‰
  - æ®‹å·®è¿æ¥ & å±‚å½’ä¸€åŒ–

### è§£ç å™¨ï¼ˆDecoderï¼‰

- åŒæ ·ç”± N ä¸ª Transformer Block ç»„æˆ
- æ¯ä¸ª Block åŒ…å«ï¼š
  - å¤šå¤´è‡ªæ³¨æ„åŠ›
  - å¤šå¤´ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›
  - å‰é¦ˆç¥ç»ç½‘ç»œ

---

## ğŸ”„ Transformer Block è¯¦è§£

```python
import torch
import torch.nn as nn
import math

class TransformerBlock(nn.Module):
    """Transformer ç¼–ç å™¨/è§£ç å™¨å—"""
    
    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()
        
        # å¤šå¤´æ³¨æ„åŠ›
        self.attention = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x, mask=None):
        # æ³¨æ„åŠ› + æ®‹å·®
        attn_output, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆ + æ®‹å·®
        ffn_output = self.ffn(x)
        x = self.norm2(x + ffn_output)
        
        return x
```

---

## ğŸ“Š æ¶æ„å¯¹æ¯”

| æ¨¡å‹ | ç¼–ç å™¨ | è§£ç å™¨ | å…¸å‹åº”ç”¨ |
|------|--------|--------|----------|
| **Transformer** | âœ“ | âœ“ | æœºå™¨ç¿»è¯‘ |
| **BERT** | âœ“ | âœ— | æ–‡æœ¬åˆ†ç±»/åºåˆ—æ ‡æ³¨ |
| **GPT** | âœ— | âœ“ | æ–‡æœ¬ç”Ÿæˆ |
| **T5** | âœ“ | âœ“ | æ–‡æœ¬åˆ°æ–‡æœ¬ |

---

## ğŸ’¡ ç®€å•ç¤ºä¾‹ï¼šä½¿ç”¨ Transformers åº“

```python
from transformers import AutoTokenizer, AutoModel

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model_name = "bert-base-chinese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ç¼–ç è¾“å…¥
text = "ä»Šå¤©å¤©æ°”çœŸå¥½"
inputs = tokenizer(text, return_tensors="pt")

# å‰å‘ä¼ æ’­
outputs = model(**inputs)

print(f"è¾“å…¥: {text}")
print(f"éšè—å±‚ç»´åº¦: {outputs.last_hidden_state.shape}")
# è¾“å‡º: torch.Size([1, 5, 768])
```

---

## ğŸ“ˆ å‘å±•å†ç¨‹

```
2017 â†’ Transformer è®ºæ–‡å‘è¡¨
   â†“
2018 â†’ BERT (Google) - é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼
   â†“
2018 â†’ GPT-1 (OpenAI) - ç”Ÿæˆå¼é¢„è®­ç»ƒ
   â†“
2019 â†’ GPT-2 - æ›´å¤§æ¨¡å‹ã€é›¶æ ·æœ¬å­¦ä¹ 
   â†“
2020 â†’ GPT-3 - few-shot å­¦ä¹ èƒ½åŠ›
   â†“
2022 â†’ ChatGPT - äººç±»å¯¹é½
   â†“
2023 â†’ GPT-4 - å¤šæ¨¡æ€èƒ½åŠ›
```

---

## ğŸ¯ å°ç»“

1. **Transformer** æ˜¯ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€æ¶æ„
2. æ ¸å¿ƒç»„ä»¶æ˜¯**æ³¨æ„åŠ›æœºåˆ¶**ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†åºåˆ—
3. è¡ç”Ÿå‡º BERTï¼ˆç¼–ç å™¨ï¼‰ã€GPTï¼ˆè§£ç å™¨ï¼‰ç­‰é‡è¦æ¨¡å‹
4. æ¨åŠ¨äº† NLP é¢†åŸŸçš„èŒƒå¼è½¬å˜

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸå§‹è®ºæ–‡
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - å›¾è§£ Transformer
- [Harvard NLP Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - ä»£ç å®ç°

---

*ğŸ”œ ä¸‹ä¸€ç« ï¼š[02_æ³¨æ„åŠ›æœºåˆ¶](./02_æ³¨æ„åŠ›æœºåˆ¶.md)*
