# 基础概念

本文档介绍大模型学习过程中需要掌握的基本术语和核心概念，适合初学者入门阅读。

---

## 1. 什么是大语言模型（LLM）

**大语言模型**（Large Language Model，简称 LLM）是一种经过大规模文本数据训练的人工智能模型，能够理解和生成人类语言。

简单来说，LLM 就像一个"超级阅读者"，它阅读了海量的书籍、网页、代码等文本，学会了理解和生成文字的能力。

### LLM 的核心能力

| 能力 | 说明 |
|------|------|
| **文本生成** | 根据提示续写文章、回答问题、创作内容 |
| **理解理解** | 理解用户意图，提取关键信息 |
| **翻译转换** | 语言翻译、格式转换、代码编写 |
| **逻辑推理** | 进行简单的数学计算和逻辑分析 |

---

## 2. 基本术语

### 2.1 Token（词元）

**Token** 是模型处理文本的基本单位。一个 token 可以是一个单词、一个汉字、一个标点符号，或者一个子词（subword）。

```
示例句子："今天天气很好"
可能被分割为：["今天", "天气", "很", "好"]
```

**为什么要了解 Token？**
- API 调用是按照 token 数量计费的
- 模型有上下文长度限制，也是用 token 来衡量的

### 2.2 Embedding（嵌入）

**Embedding** 是将文字转换为数字向量（一组数字）的技术，它是模型理解文字含义的方式。

```
文字 → 数字向量
"猫"   → [0.12, -0.34, 0.56, ...]
"狗"   → [0.11, -0.32, 0.58, ...]   # 与"猫"的向量比较接近
"汽车" → [0.89, 0.12, -0.45, ...]  # 与"猫"的向量相差较远
```

### 2.3 参数（Parameters）

**参数**是模型在训练过程中学习到的"知识"，通常用"有多少亿参数"来描述模型的大小。

- **参数越多**，通常意味着模型越强大，但需要更多的计算资源
- 常见模型参数规模：7B（70亿）、13B、70B、100B+ 等

### 2.4 上下文长度（Context Length）

**上下文长度**是指模型一次能处理的 token 数量上限。

- 超过这个长度，早期的内容可能被"遗忘"
- 较长的上下文允许模型处理更长的文档、代码

---

## 3. 模型相关概念

### 3.1 预训练（Pretraining）

**预训练**是大模型学习"语言能力"的阶段，就像学生的基础教育阶段。

在这个阶段，模型通过阅读海量文本，学会：
- 语言的语法和结构
- 世界的常识知识
- 基本的推理能力

### 3.2 微调（Fine-tuning）

**微调**是在预训练模型基础上，用特定数据进行进一步训练，使模型适应特定任务。

```
预训练模型：通用知识 → 微调 → 特定领域专家
（如：ChatGPT）    （如：医疗助手、法律顾问）
```

### 3.3 对齐（Alignment）

**对齐**是指让模型的输出符合人类价值观和期望，避免产生有害内容。

常见的对齐技术：
- RLHF（基于人类反馈的强化学习）
- DPO（直接偏好优化）

---

## 4. 推理参数

### 4.1 Temperature（温度）

**Temperature** 控制输出的随机性：

| 值 | 效果 |
|----|------|
| **低（0.1-0.3）** | 输出更确定性，重复性高 |
| **中（0.7-1.0）** | 平衡创造性和确定性 |
| **高（>1.0）** | 输出更随机、更有创意 |

简单理解：温度越高，模型越"随机应变"；温度越低，模型越"保守稳定"。

### 4.2 Top-P（核采样）

**Top-P** 是另一种控制随机性的参数。

它表示模型只从概率最高的 P 比例的候选词中选择：

```
Top-P = 0.9：只从累计概率达90%的词中选择
Top-P = 0.99：选择范围更大，更随机
```

### 4.3 Top-K

**Top-K** 限制模型只考虑概率最高的 K 个词：

```
Top-K = 1：始终选择最可能的词（最确定性）
Top-K = 50：考虑概率最高的50个词
```

---

## 5. 模型架构

### 5.1 Transformer

**Transformer** 是现代大模型的核心架构，几乎所有主流大模型都基于 Transformer。

```
Transformer = 编码器(Encoder) + 解码器(Decoder)
```

### 5.2 Decoder-Only（纯解码器）

目前大多数对话模型采用的架构，如 GPT 系列、LLaMA 等。

特点：单向注意力，只能看到前面的内容

### 5.3 Encoder-Decoder（编码器-解码器）

如 T5、BART 等模型架构。

特点：双向注意力，可以同时看到前后内容

---

## 6. 常见模型类型

| 类型 | 代表模型 | 特点 |
|------|----------|------|
| **通用对话** | GPT-4、Claude、 Gemini | 通用能力强 |
| **开源模型** | LLaMA、Qwen、DeepSeek | 可本地部署 |
| **代码模型** | CodeLlama、DeepSeek-Coder | 编程能力强 |
| **多模态模型** | GPT-4V、Claude 3 | 支持图像理解 |

---

## 7. 性能评估指标

### 7.1 困惑度（Perplexity）

衡量模型对文本的"困惑"程度，**越低越好**。

### 7.2 BLEU / ROUGE

机器翻译和文本生成常用的评估指标。

### 7.3 MMLU / C-Eval

大模型知识推理能力的基准测试。

---

## 8. 总结

| 概念 | 核心要点 |
|------|----------|
| Token | 模型处理的最小单位 |
| Embedding | 文字的数字表示 |
| 参数 | 模型大小的衡量标准 |
| 预训练 | 学习通用语言能力 |
| 微调 | 适应特定任务 |
| Temperature | 控制输出随机性 |

---

## 📎 相关资源

- [01_入门指引](../01_入门指引/) - 了解大模型发展历程
- [03_技术原理](../03_技术原理/) - 深入学习 Transformer 架构

---

*📝 更新日期：2026-02-24*
