# å®æˆ˜æ¡ˆä¾‹ï¼šä½¿ç”¨ QLoRA è¿›è¡Œå¤§æ¨¡å‹å¾®è°ƒå®æˆ˜

> æœ¬æ–‡å°†å¸¦æ‚¨ä»é›¶å¼€å§‹ï¼Œä½¿ç”¨ QLoRA æŠ€æœ¯åœ¨æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒå¤§è¯­è¨€æ¨¡å‹ï¼Œå®ç°ä¸ªæ€§åŒ–æ¨¡å‹è®­ç»ƒã€‚

---

## ğŸ“‹ æ¡ˆä¾‹æ¦‚è¿°

### åœºæ™¯
ä¸ªäººå¼€å‘è€…æˆ–å°å‹å›¢é˜Ÿéœ€è¦åœ¨æœ¬åœ°ç¯å¢ƒå¾®è°ƒå¤§æ¨¡å‹ï¼Œä½†å—é™äºæ˜¾å­˜é¢„ç®—ï¼š
- ç›®æ ‡ï¼šåœ¨ 24GB æ˜¾å­˜çš„æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒ 7B~13B å‚æ•°çš„æ¨¡å‹
- éœ€æ±‚ï¼šè®­ç»ƒè‡ªå·±çš„å¯¹è¯æ¨¡å‹ / å‚ç›´é¢†åŸŸä¸“å®¶æ¨¡å‹
- é™åˆ¶ï¼šæ˜¾å­˜æœ‰é™ï¼Œæ— é«˜ç«¯æœåŠ¡å™¨

### æŠ€æœ¯æ ˆ
- **åŸºç¡€æ¨¡å‹**ï¼šLlama2-7B / Qwen-7B / ChatGLM3-6B
- **å¾®è°ƒæŠ€æœ¯**ï¼šQLoRA (é‡åŒ– + LoRA)
- **æ¡†æ¶**ï¼šTransformers + PEFT + bitsandbytes
- **è®­ç»ƒå·¥å…·**ï¼šDeepSpeed / SFTTrainer

### å­¦ä¹ ç›®æ ‡
1. æŒæ¡ QLoRA æŠ€æœ¯çš„å®Œæ•´å·¥ä½œæµç¨‹
2. å­¦ä¼šé…ç½®å’Œä¼˜åŒ–å¾®è°ƒå‚æ•°
3. èƒ½å¤Ÿç‹¬ç«‹å®Œæˆæ¨¡å‹å¾®è°ƒå…¨æµç¨‹
4. äº†è§£å¸¸è§é—®é¢˜çš„æ’æŸ¥å’Œè§£å†³

---

## ğŸ—ï¸ æ¡ˆä¾‹é¡¹ç›®ç»“æ„

```
qlora-finetune-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl          # è®­ç»ƒæ•°æ®
â”‚   â””â”€â”€ eval.jsonl           # éªŒè¯æ•°æ®
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py             # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ inference.py         # æ¨ç†è„šæœ¬
â”‚   â””â”€â”€ merge_model.py       # æƒé‡åˆå¹¶è„šæœ¬
â”œâ”€â”€ config/
â”‚   â””â”€â”€ qlora_config.py      # é…ç½®æ–‡ä»¶
â”œâ”€â”€ output/
â”‚   â””â”€â”€ checkpoints/        # æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
â”œâ”€â”€ .env                     # API å¯†é’¥é…ç½®
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â””â”€â”€ README.md
```

---

## ğŸš€ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚

| GPU æ˜¾å­˜ | å¯å¾®è°ƒæ¨¡å‹ | æ‰¹å¤§å° |
|---------|-----------|--------|
| 16GB | 7B æ¨¡å‹ | 1-2 |
| 24GB | 7B~13B æ¨¡å‹ | 2-4 |
| 40GB+ | 13B~70B æ¨¡å‹ | 4-8 |

### 1.2 åˆ›å»ºé¡¹ç›®ç›®å½•

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p qlora-finetune-project/{data,scripts,config,output/checkpoints}
cd qlora-finetune-project
```

### 1.3 å®‰è£…ä¾èµ–

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# å®‰è£…æ ¸å¿ƒä¾èµ–
pip install torch==2.1.0
pip install transformers>=4.35.0
pip install peft>=0.7.0
pip install bitsandbytes>=0.41.0
pip install accelerate>=0.25.0
pip install datasets>=2.14.0
pip install trl>=0.7.0
pip install scipy>=1.11.0

# å®‰è£… HuggingFace Hubï¼ˆç”¨äºä¸‹è½½æ¨¡å‹ï¼‰
pip install huggingface-hub
```

### 1.4 ä¸‹è½½æ¨¡å‹

```bash
# ä½¿ç”¨ HuggingFace CLI ç™»å½•ï¼ˆéœ€è¦å…ˆåœ¨ https://huggingface.co/settings/tokens è·å– tokenï¼‰
huggingface-cli login

# æˆ–è€…ç›´æ¥ä»£ç ç™»å½•
from huggingface_hub import login
login(token="your_huggingface_token")

# ä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½ï¼‰
# Llama2-7B (éœ€è¦ç”³è¯·è®¿é—®æƒé™)
# Qwen-7B (å¼€æºå¯ç›´æ¥ä¸‹è½½)
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "Qwen/Qwen-7B-Chat"  # ä½¿ç”¨å¼€æºæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(
    model_name, 
    trust_remote_code=True
)
print("Tokenizer ä¸‹è½½å®Œæˆ")
```

---

## ğŸ“ ç¬¬äºŒæ­¥ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®

### 2.1 æ•°æ®æ ¼å¼

QLoRA å¾®è°ƒæ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼Œæ¨èä½¿ç”¨æŒ‡ä»¤å¾®è°ƒæ ¼å¼ï¼š

```json
{
  "instruction": "è¯·ä»‹ç»ä¸€ä¸‹åŒ—äº¬çš„å†å²",
  "input": "",
  "output": "åŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½ï¼Œæœ‰ç€ä¸‰åƒå¤šå¹´çš„å†å²..."
}
```

æˆ–å¯¹è¯æ ¼å¼ï¼š

```json
{
  "conversations": [
    {"role": "user", "content": "ä½ å¥½"},
    {"role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"}
  ]
}
```

### 2.2 å‡†å¤‡ç¤ºä¾‹æ•°æ®

åˆ›å»º `data/train.jsonl`ï¼š

```jsonl
{"instruction": "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½", "input": "", "output": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºå¼€å‘èƒ½å¤Ÿæ¨¡æ‹Ÿã€å»¶ä¼¸å’Œæ‰©å±•äººç±»æ™ºèƒ½çš„ç³»ç»Ÿã€‚å®ƒåŒ…æ‹¬æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰æŠ€æœ¯ã€‚"}
{"instruction": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "input": "", "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºå¼€å‘èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›çš„ç®—æ³•ã€‚å®ƒä¸éœ€è¦æ˜ç¡®ç¼–ç¨‹ï¼Œè€Œæ˜¯é€šè¿‡å¤§é‡æ•°æ®è®­ç»ƒæ¨¡å‹æ¥å®Œæˆä»»åŠ¡ã€‚"}
{"instruction": "è§£é‡Šæ·±åº¦å­¦ä¹ ", "input": "", "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰æ¥å­¦ä¹ æ•°æ®çš„å±‚æ¬¡åŒ–è¡¨ç¤ºã€‚å®ƒåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚"}
{"instruction": "Pythonæœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ", "input": "", "output": "Pythonçš„ä¼˜åŠ¿åŒ…æ‹¬ï¼š1.ç®€æ´æ˜“è¯»çš„è¯­æ³• 2.ä¸°å¯Œçš„æ ‡å‡†åº“å’Œç¬¬ä¸‰æ–¹åŒ… 3.å¹¿æ³›çš„åº”ç”¨é¢†åŸŸ 4.å¼ºå¤§çš„ç¤¾åŒºæ”¯æŒ 5.è·¨å¹³å°å…¼å®¹æ€§ 6.æ˜“äºå­¦ä¹ å’Œå…¥é—¨"}
{"instruction": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ", "input": "", "output": "å­¦ä¹ ç¼–ç¨‹çš„å»ºè®®ï¼š1.é€‰æ‹©ä¸€é—¨å…¥é—¨è¯­è¨€ï¼ˆå¦‚Pythonï¼‰2.å­¦ä¹ åŸºç¡€è¯­æ³• 3.åŠ¨æ‰‹å®è·µå°é¡¹ç›® 4.é˜…è¯»ä¼˜ç§€ä»£ç  5.å‚ä¸å¼€æºé¡¹ç›® 6.åšæŒç¼–ç ç»ƒä¹ "}
```

åˆ›å»º `data/eval.jsonl`ï¼š

```jsonl
{"instruction": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ", "input": "", "output": "ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å—ç”Ÿç‰©å¤§è„‘å¯å‘çš„è®¡ç®—æ¨¡å‹ï¼Œç”±å¤šå±‚ç¥ç»å…ƒç»„æˆï¼Œç”¨äºå­¦ä¹ å¤æ‚çš„æ¨¡å¼å’Œéçº¿æ€§å…³ç³»ã€‚"}
{"instruction": "è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹", "input": "", "output": "å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ˜¯åŸºäºTransformeræ¶æ„çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡æµ·é‡æ–‡æœ¬è®­ç»ƒï¼Œå…·å¤‡å¼ºå¤§çš„è¯­è¨€ç†è§£å’Œç”Ÿæˆèƒ½åŠ›ã€‚"}
```

### 2.3 æ•°æ®é¢„å¤„ç†è„šæœ¬

åˆ›å»º `scripts/prepare_data.py`ï¼š

```python
"""æ•°æ®é¢„å¤„ç†è„šæœ¬ï¼šå°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼"""

import json
import os
from datasets import Dataset


def load_jsonl(file_path: str):
    """åŠ è½½ JSONL æ ¼å¼æ•°æ®"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data


def format_instruction(sample):
    """æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®ä¸ºè®­ç»ƒæ–‡æœ¬"""
    # æŒ‡ä»¤å¾®è°ƒæ¨¡æ¿
    text = f"""### æŒ‡ä»¤
{sample['instruction']}

### å›ç­”
{sample['output']}

"""
    return {"text": text}


def format_chat(sample):
    """æ ¼å¼åŒ–å¯¹è¯æ•°æ®ä¸ºè®­ç»ƒæ–‡æœ¬"""
    if "conversations" in sample:
        text = ""
        for msg in sample["conversations"]:
            if msg["role"] == "user":
                text += f"User: {msg['content']}\n"
            elif msg["role"] == "assistant":
                text += f"Assistant: {msg['content']}\n"
        return {"text": text}
    return sample


def prepare_dataset(data_path: str, output_path: str = None):
    """å‡†å¤‡æ•°æ®é›†"""
    # åŠ è½½æ•°æ®
    raw_data = load_jsonl(data_path)
    print(f"åŠ è½½äº† {len(raw_data)} æ¡æ•°æ®")
    
    # åˆ›å»º Dataset
    dataset = Dataset.from_list(raw_data)
    
    # æ ¼å¼åŒ–
    if "conversations" in raw_data[0]:
        dataset = dataset.map(format_chat)
    else:
        dataset = dataset.map(format_instruction)
    
    # æ‰“å°æ ·ä¾‹
    print("æ•°æ®æ ·ä¾‹:")
    print(dataset[0]["text"][:200])
    
    # ä¿å­˜
    if output_path:
        dataset.save_to_disk(output_path)
        print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {output_path}")
    
    return dataset


if __name__ == "__main__":
    # å¤„ç†è®­ç»ƒæ•°æ®
    train_data = prepare_dataset("data/train.jsonl")
    
    # å¤„ç†éªŒè¯æ•°æ®
    eval_data = prepare_dataset("data/eval.jsonl")
    
    print("\næ•°æ®å‡†å¤‡å®Œæˆ!")
```

---

## âš™ï¸ ç¬¬ä¸‰æ­¥ï¼šé…ç½® QLoRA å‚æ•°

åˆ›å»º `config/qlora_config.py`ï¼š

```python
"""QLoRA é…ç½®æ–‡ä»¶"""

from dataclasses import dataclass
from typing import Optional, List


@dataclass
class QLoRAConfig:
    # æ¨¡å‹é…ç½®
    model_name: str = "Qwen/Qwen-7B-Chat"
    model_path: Optional[str] = None  # æœ¬åœ°æ¨¡å‹è·¯å¾„
    
    # LoRA é…ç½®
    lora_r: int = 16  # LoRA ç§©
    lora_alpha: int = 32  # LoRA ç¼©æ”¾å‚æ•°
    lora_dropout: float = 0.05  # Dropout æ¦‚ç‡
    target_modules: List[str] = None  # ç›®æ ‡æ¨¡å—
    
    # é‡åŒ–é…ç½®
    load_in_4bit: bool = True
    bnb_4bit_quant_type: str = "nf4"  # nf4 æˆ– fp4
    bnb_4bit_compute_dtype: str = "float16"  # è®¡ç®—ç²¾åº¦
    bnb_4bit_use_double_quant: bool = True
    
    # è®­ç»ƒé…ç½®
    output_dir: str = "output/checkpoints"
    num_train_epochs: int = 3
    per_device_train_batch_size: int = 2
    per_device_eval_batch_size: int = 1
    gradient_accumulation_steps: int = 4
    learning_rate: float = 3e-4
    max_seq_length: int = 512
    warmup_steps: int = 100
    logging_steps: int = 10
    save_steps: int = 100
    eval_steps: int = 100
    save_total_limit: int = 3
    
    # ä¼˜åŒ–å™¨é…ç½®
    optim: str = "paged_adamw_8bit"
    fp16: bool = True
    bf16: bool = False
    
    # å…¶ä»–
    seed: int = 42
    dataloader_num_workers: int = 4
    
    def __post_init__(self):
        if self.target_modules is None:
            # Qwen/Qwen2 æ¨¡å‹
            if "qwen" in self.model_name.lower():
                self.target_modules = [
                    "q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"
                ]
            # Llama æ¨¡å‹
            elif "llama" in self.model_name.lower():
                self.target_modules = [
                    "q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"
                ]
            # ChatGLM æ¨¡å‹
            elif "chatglm" in self.model_name.lower():
                self.target_modules = [
                    "query_key_value", "dense", "dense_h_to_4h", "dense_4h_to_h"
                ]
            else:
                # é»˜è®¤é…ç½®
                self.target_modules = ["q_proj", "k_proj", "v_proj", "o_proj"]


# å¸¸ç”¨æ¨¡å‹é…ç½®æ¨¡æ¿
MODEL_CONFIGS = {
    "qwen-7b": {
        "model_name": "Qwen/Qwen-7B-Chat",
        "lora_r": 16,
        "per_device_train_batch_size": 2,
    },
    "qwen-14b": {
        "model_name": "Qwen/Qwen-14B-Chat",
        "lora_r": 16,
        "per_device_train_batch_size": 1,
    },
    "llama2-7b": {
        "model_name": "meta-llama/Llama-2-7b-chat-hf",
        "lora_r": 16,
        "per_device_train_batch_size": 2,
    },
    "llama2-13b": {
        "model_name": "meta-llama/Llama-2-13b-chat-hf",
        "lora_r": 16,
        "per_device_train_batch_size": 1,
    },
    "chatglm3-6b": {
        "model_name": "THUDM/chatglm3-6b",
        "lora_r": 8,
        "per_device_train_batch_size": 2,
    },
}


def get_config(model_size: str = "qwen-7b", **kwargs) -> QLoRAConfig:
    """è·å–é¢„å®šä¹‰çš„æ¨¡å‹é…ç½®"""
    base_config = MODEL_CONFIGS.get(model_size, MODEL_CONFIGS["qwen-7b"])
    config = QLoRAConfig(**base_config)
    
    # å…è®¸è¦†ç›–é…ç½®
    for key, value in kwargs.items():
        if hasattr(config, key):
            setattr(config, key, value)
    
    return config
```

---

## ğŸ‹ï¸ ç¬¬å››æ­¥ï¼šç¼–å†™è®­ç»ƒè„šæœ¬

åˆ›å»º `scripts/train.py`ï¼š

```python
"""
QLoRA è®­ç»ƒè„šæœ¬
ä½¿ç”¨ SFTTrainer è¿›è¡Œç›‘ç£å¾®è°ƒ
"""

import os
import sys
import torch
from dataclasses import asdict
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    DataCollatorForLanguageModeling,
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_kbit_training,
)
from datasets import load_dataset
from trl import SFTTrainer

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.qlora_config import QLoRAConfig, get_config


def setup_model_and_tokenizer(config: QLoRAConfig):
    """åŠ è½½é‡åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨"""
    print("=" * 50)
    print("åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    print("=" * 50)
    
    # 1. é…ç½® 4-bit é‡åŒ–
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=config.load_in_4bit,
        bnb_4bit_quant_type=config.bnb_4bit_quant_type,
        bnb_4bit_compute_dtype=getattr(torch, config.bnb_4bit_compute_dtype),
        bnb_4bit_use_double_quant=config.bnb_4bit_use_double_quant,
    )
    
    # 2. åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        config.model_name,
        trust_remote_code=True,
        padding_side="right",
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # 3. åŠ è½½é‡åŒ–æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        config.model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # 4. å‡†å¤‡æ¨¡å‹è¿›è¡Œé‡åŒ–è®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    print(f"æ¨¡å‹åŠ è½½å®Œæˆ: {config.model_name}")
    print(f"é‡åŒ–é…ç½®: {config.bnb_4bit_quant_type}")
    
    return model, tokenizer


def setup_lora_config(config: QLoRAConfig):
    """é…ç½® LoRA å‚æ•°"""
    lora_config = LoraConfig(
        r=config.lora_r,
        lora_alpha=config.lora_alpha,
        lora_dropout=config.lora_dropout,
        target_modules=config.target_modules,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
    )
    return lora_config


def load_training_data(config: QLoRAConfig):
    """åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®"""
    print("\nåŠ è½½è®­ç»ƒæ•°æ®...")
    
    # åŠ è½½ JSONL æ ¼å¼æ•°æ®
    train_dataset = load_dataset(
        "json",
        data_files="data/train.jsonl",
        split="train"
    )
    
    eval_dataset = load_dataset(
        "json",
        data_files="data/eval.jsonl",
        split="train"
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
    print(f"éªŒè¯é›†å¤§å°: {len(eval_dataset)}")
    
    return train_dataset, eval_dataset


def formatting_prompts_func(example, tokenizer):
    """æ ¼å¼åŒ–è®­ç»ƒæ ·æœ¬"""
    # æŒ‡ä»¤å¾®è°ƒæ ¼å¼
    text = f"""### æŒ‡ä»¤
{example['instruction']}

### å›ç­”
{example['output']}

"""
    return {"text": text}


def main():
    # 1. è·å–é…ç½®
    config = get_config("qwen-7b")
    print("è®­ç»ƒé…ç½®:")
    for key, value in asdict(config).items():
        print(f"  {key}: {value}")
    
    # 2. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config.output_dir, exist_ok=True)
    
    # 3. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = setup_model_and_tokenizer(config)
    
    # 4. åº”ç”¨ LoRA
    print("\nåº”ç”¨ LoRA...")
    lora_config = setup_lora_config(config)
    model = get_peft_model(model, lora_config)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°
    model.print_trainable_parameters()
    
    # 5. åŠ è½½æ•°æ®
    train_dataset, eval_dataset = load_training_data(config)
    
    # 6. é…ç½®è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=config.output_dir,
        num_train_epochs=config.num_train_epochs,
        per_device_train_batch_size=config.per_device_train_batch_size,
        per_device_eval_batch_size=config.per_device_eval_batch_size,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        learning_rate=config.learning_rate,
        warmup_steps=config.warmup_steps,
        logging_steps=config.logging_steps,
        save_steps=config.save_steps,
        eval_steps=config.eval_steps,
        save_total_limit=config.save_total_limit,
        fp16=config.fp16,
        bf16=config.bf16,
        optim=config.optim,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        report_to="none",
        remove_unused_columns=False,
        ddp_find_unused_parameters=False,
    )
    
    # 7. åˆ›å»ºæ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
    )
    
    # 8. åˆ›å»º SFTTrainer
    print("\nå¼€å§‹è®­ç»ƒ...")
    print("=" * 50)
    
    trainer = SFTTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        formatting_func=lambda x: formatting_prompts_func(x, tokenizer),
        max_seq_length=config.max_seq_length,
        peft_config=lora_config,
    )
    
    # 9. å¼€å§‹è®­ç»ƒ
    train_result = trainer.train()
    
    # 10. ä¿å­˜æ¨¡å‹
    print("\nä¿å­˜æ¨¡å‹...")
    trainer.save_model(config.output_dir)
    trainer.save_state()
    tokenizer.save_pretrained(config.output_dir)
    
    # æ‰“å°è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    print("\n" + "=" * 50)
    print("è®­ç»ƒå®Œæˆ!")
    print(f"æ¨¡å‹ä¿å­˜è·¯å¾„: {config.output_dir}")
    print("=" * 50)


if __name__ == "__main__":
    main()
```

---

## ğŸ”¥ ç¬¬äº”æ­¥ï¼šè¿è¡Œè®­ç»ƒ

### 5.1 å•å¡è®­ç»ƒ

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# è¿è¡Œè®­ç»ƒ
python scripts/train.py
```

### 5.2 å¤šå¡è®­ç»ƒï¼ˆåˆ†å¸ƒå¼ï¼‰

```bash
# ä½¿ç”¨ DeepSpeed è¿›è¡Œå¤šå¡è®­ç»ƒ
deepspeed --num_gpus=2 scripts/train.py

# æˆ–ä½¿ç”¨ Accelerate
accelerate launch --num_gpus=2 scripts/train.py
```

### 5.3 è®­ç»ƒè¿‡ç¨‹ç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºä»¥ä¸‹ä¿¡æ¯ï¼š

```
==================================================
åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...
==================================================
æ¨¡å‹åŠ è½½å®Œæˆ: Qwen/Qwen-7B-Chat
é‡åŒ–é…ç½®: nf4

åº”ç”¨ LoRA...
trainable params: 5,242,880 || all params: 7,744,000,000 || trainable%: 0.0677

åŠ è½½è®­ç»ƒæ•°æ®...
è®­ç»ƒé›†å¤§å°: 5
éªŒè¯é›†å¤§å°: 2

==================================================
å¼€å§‹è®­ç»ƒ...
==================================================
{'loss': 2.3456, 'learning_rate': 0.0003, 'epoch': 0.33}
{'loss': 1.8765, 'learning_rate': 0.0003, 'epoch': 0.67}
{'loss': 1.5432, 'learning_rate': 0.0002, 'epoch': 1.00}
...
==================================================
è®­ç»ƒå®Œæˆ!
æ¨¡å‹ä¿å­˜è·¯å¾„: output/checkpoints
==================================================
```

### 5.4 æ˜¾å­˜ä¼˜åŒ–æŠ€å·§

å¦‚æœé‡åˆ°æ˜¾å­˜ä¸è¶³é—®é¢˜ï¼š

```python
# åœ¨ config ä¸­è°ƒæ•´
config = QLoRAConfig(
    # 1. å‡å°æ‰¹å¤§å°
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,  # é€šè¿‡æ¢¯åº¦ç´¯ç§¯å¼¥è¡¥
    
    # 2. å‡å°åºåˆ—é•¿åº¦
    max_seq_length=256,
    
    # 3. ä½¿ç”¨æ›´ä½çš„é‡åŒ–ç²¾åº¦
    bnb_4bit_quant_type="fp4",  # æˆ–ä½¿ç”¨ 8bit
    
    # 4. å¸è½½ä¼˜åŒ–å™¨åˆ° CPU
    optim="paged_adamw_32bit",
)
```

---

## ğŸ¤– ç¬¬å…­æ­¥ï¼šæ¨¡å‹æ¨ç†

åˆ›å»º `scripts/inference.py`ï¼š

```python
"""
QLoRA å¾®è°ƒæ¨¡å‹æ¨ç†è„šæœ¬
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    GenerationConfig,
)
from peft import PeftModel
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def load_model_and_tokenizer(
    base_model_name: str = "Qwen/Qwen-7B-Chat",
    checkpoint_path: str = "output/checkpoints",
    load_in_4bit: bool = True,
):
    """åŠ è½½å¾®è°ƒåçš„æ¨¡å‹"""
    
    # é‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )
    
    # åŠ è½½ LoRA æƒé‡
    model = PeftModel.from_pretrained(
        base_model,
        checkpoint_path,
        device_map="auto",
    )
    
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        trust_remote_code=True,
        padding_side="left",
    )
    tokenizer.eos_token
    
.pad_token = tokenizer    return model, tokenizer


def generate_response(
    model,
    tokenizer,
    instruction: str,
    input_text: str = "",
    max_new_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 20,
):
    """ç”Ÿæˆå›å¤"""
    
    # æ„å»ºæç¤ºè¯
    if input_text:
        prompt = f"""### æŒ‡ä»¤
{instruction}

### è¾“å…¥
{input_text}

### å›ç­”
"""
    else:
        prompt = f"""### æŒ‡ä»¤
{instruction}

### å›ç­”
"""
    
    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            do_sample=True,
            repetition_penalty=1.1,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡º
    response = tokenizer.decode(
        outputs[0][inputs.input_ids.shape[1]:],
        skip_special_tokens=True
    )
    
    return response.strip()


def chat_loop(model, tokenizer):
    """äº¤äº’å¼èŠå¤©å¾ªç¯"""
    print("\n" + "=" * 50)
    print("ğŸ¤– QLoRA å¾®è°ƒæ¨¡å‹èŠå¤©æœºå™¨äºº")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("=" * 50 + "\n")
    
    while True:
        user_input = input("ä½ : ").strip()
        
        if user_input.lower() in ["quit", "exit", "q"]:
            print("å†è§!")
            break
        
        if not user_input:
            continue
        
        response = generate_response(model, tokenizer, user_input)
        print(f"\nğŸ¤–: {response}\n")


def main():
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model_and_tokenizer(
        base_model_name="Qwen/Qwen-7B-Chat",
        checkpoint_path="output/checkpoints",
    )
    
    print("æ¨¡å‹åŠ è½½å®Œæˆ!")
    
    # æµ‹è¯•å‡ æ¡
    test_questions = [
        "è¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½",
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Pythonæœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ",
    ]
    
    print("\næµ‹è¯•ç»“æœ:")
    print("=" * 50)
    
    for question in test_questions:
        response = generate_response(model, tokenizer, question)
        print(f"\né—®é¢˜: {question}")
        print(f"å›ç­”: {response}\n")
    
    # å¼€å¯äº¤äº’å¼èŠå¤©
    chat_loop(model, tokenizer)


if __name__ == "__main__":
    main()
```

è¿è¡Œæ¨ç†ï¼š

```bash
python scripts/inference.py
```

---

## ğŸ”— ç¬¬ä¸ƒæ­¥ï¼šåˆå¹¶æ¨¡å‹æƒé‡ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦å°† LoRA æƒé‡åˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­ï¼š

åˆ›å»º `scripts/merge_model.py`ï¼š

```python
"""
åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹
"""

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
)
from peft import PeftModel
import sys
import os
import argparse

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def merge_model(
    base_model_name: str,
    checkpoint_path: str,
    output_path: str,
    load_in_4bit: bool = True,
):
    """åˆå¹¶ LoRA æƒé‡"""
    
    print("åŠ è½½åŸºç¡€æ¨¡å‹...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
    )
    
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        quantization_config=bnb_config if load_in_4bit else None,
        device_map="cpu",  # åœ¨ CPU ä¸Šåˆå¹¶
        trust_remote_code=True,
    )
    
    # åŠ è½½ LoRA æƒé‡
    print("åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(
        base_model,
        checkpoint_path,
        device_map="cpu",
    )
    
    # åˆå¹¶æƒé‡
    print("åˆå¹¶æƒé‡...")
    merged_model = model.merge_and_unload()
    
    # ä¿å­˜
    print(f"ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ°: {output_path}")
    merged_model.save_pretrained(output_path)
    
    # ä¿å­˜åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        trust_remote_code=True,
    )
    tokenizer.save_pretrained(output_path)
    
    print("å®Œæˆ!")


def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶ LoRA æƒé‡")
    parser.add_argument("--base_model", type=str, default="Qwen/Qwen-7B-Chat")
    parser.add_argument("--checkpoint", type=str, default="output/checkpoints")
    parser.add_argument("--output", type=str, default="output/merged_model")
    parser.add_argument("--load_in_4bit", action="store_true", default=True)
    
    args = parser.parse_args()
    
    merge_model(
        args.base_model,
        args.checkpoint,
        args.output,
        args.load_in_4bit,
    )


if __name__ == "__main__":
    main()
```

è¿è¡Œåˆå¹¶ï¼š

```bash
python scripts/merge_model.py \
    --base_model Qwen/Qwen-7B-Chat \
    --checkpoint output/checkpoints \
    --output output/merged_model
```

---

## ğŸ› ï¸ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜ 1ï¼šæ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼šOOM (Out Of Memory) é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. å‡å°æ‰¹å¤§å°
per_device_train_batch_size = 1

# 2. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
gradient_accumulation_steps = 8

# 3. å‡å°åºåˆ—é•¿åº¦
max_seq_length = 256

# 4. ä½¿ç”¨ 8bit é‡åŒ–
load_in_4bit = False
load_in_8bit = True

# 5. å¼€å¯è™šæ‹Ÿå†…å­˜è°ƒåº¦
optim = "paged_adamw_32bit"
```

### é—®é¢˜ 2ï¼šè®­ç»ƒæ•ˆæœå·®

**ç—‡çŠ¶**ï¼šæ¨¡å‹è¾“å‡ºä¸ç†æƒ³ï¼Œloss ä¸ä¸‹é™

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
# ç¡®ä¿ instruction å’Œ output å­—æ®µæ­£ç¡®

# 2. è°ƒæ•´å­¦ä¹ ç‡
learning_rate = 1e-4  # å°è¯•æ›´å°çš„å­¦ä¹ ç‡

# 3. å¢åŠ  LoRA ç§©
lora_r = 32  # ä» 16 å¢åŠ åˆ° 32

# 4. å¢åŠ è®­ç»ƒè½®æ•°
num_train_epochs = 5
```

### é—®é¢˜ 3ï¼šæ¨¡å‹ä¸æ”¶æ•›

**ç—‡çŠ¶**ï¼šéªŒè¯é›† loss æŒç»­ä¸Šå‡

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. æ·»åŠ  warmup
warmup_steps = 100

# 2. ä½¿ç”¨æ›´å¥½çš„ä¼˜åŒ–å™¨
optim = "paged_adamw_8bit"

# 3. æ·»åŠ æ­£åˆ™åŒ–
lora_dropout = 0.1
```

### é—®é¢˜ 4ï¼šæ¨ç†é€Ÿåº¦æ…¢

**ç—‡çŠ¶**ï¼šç”Ÿæˆé€Ÿåº¦å¾ˆæ…¢

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# 1. åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹
# è§ scripts/merge_model.py

# 2. ä½¿ç”¨é‡åŒ–æ¨ç†
load_in_4bit = True

# 3. å¯ç”¨ KV Cache
model = AutoModelForCausalLM.from_pretrained(
    ...,
    use_cache=True,  # é»˜è®¤å¯ç”¨
)
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

ä»¥ä¸‹æ˜¯æˆ‘ä»¬ä½¿ç”¨ä¸åŒé…ç½®æµ‹è¯•çš„ç»“æœï¼š

| æ¨¡å‹ | GPU | é‡åŒ– | æ‰¹å¤§å° | æ˜¾å­˜å ç”¨ | è®­ç»ƒé€Ÿåº¦ |
|------|-----|------|--------|---------|---------|
| Qwen-7B | RTX 3090 (24GB) | 4bit | 2 | ~18GB | ~100 steps/h |
| Qwen-7B | RTX 3090 (24GB) | 4bit | 4 | ~22GB | ~80 steps/h |
| Llama2-7B | RTX 3090 (24GB) | 4bit | 2 | ~20GB | ~90 steps/h |
| Llama2-13B | RTX 4090 (24GB) | 4bit | 1 | ~22GB | ~40 steps/h |

---

## ğŸ“š æ‰©å±•å­¦ä¹ 

### æ¨èé˜…è¯»
1. [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)
2. [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
3. [PEFT: Parameter-Efficient Fine-Tuning](https://github.com/huggingface/peft)

### è¿›é˜¶ä¸»é¢˜
- ä½¿ç”¨ DeepSpeed ZeRO è¿›è¡Œæ›´å¤§è§„æ¨¡è®­ç»ƒ
- æ¢ç´¢ä¸åŒçš„é‡åŒ–æ–¹æ³• (GPTQ, AWQ)
- å¤šæ¨¡æ€æ¨¡å‹çš„å¾®è°ƒ
- RLHF (äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ )

---

## âœ… æ€»ç»“

é€šè¿‡æœ¬å®æˆ˜æ¡ˆä¾‹ï¼Œæ‚¨åº”è¯¥å·²ç»æŒæ¡äº†ï¼š

1. **ç¯å¢ƒæ­å»º**ï¼šé…ç½® QLoRA å¾®è°ƒç¯å¢ƒ
2. **æ•°æ®å‡†å¤‡**ï¼šå‡†å¤‡å’Œæ ¼å¼åŒ–è®­ç»ƒæ•°æ®
3. **æ¨¡å‹é…ç½®**ï¼šè®¾ç½® LoRA å’Œé‡åŒ–å‚æ•°
4. **è®­ç»ƒæµç¨‹**ï¼šå®Œæ•´è®­ç»ƒæµç¨‹å’Œç›‘æ§
5. **æ¨¡å‹æ¨ç†**ï¼šä½¿ç”¨å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†
6. **æƒé‡åˆå¹¶**ï¼šåˆå¹¶ LoRA æƒé‡ï¼ˆå¯é€‰ï¼‰

QLoRA æŠ€æœ¯ä½¿å¾—åœ¨æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒå¤§æ¨¡å‹æˆä¸ºå¯èƒ½ï¼Œå¤§å¤§é™ä½äº† AI å¼€å‘çš„é—¨æ§›ã€‚å¸Œæœ›æœ¬æ•™ç¨‹èƒ½å¸®åŠ©æ‚¨è®­ç»ƒå‡ºå±äºè‡ªå·±çš„ä¸ªæ€§åŒ–æ¨¡å‹ï¼

---

## ğŸ”— ç›¸å…³èµ„æº

- [PEFT å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs/peft)
- [bitsandbytes åº“](https://github.com/TimDettmers/bitsandbytes)
- [TRL åº“](https://huggingface.co/docs/trl)
- [Qwen æ¨¡å‹åº“](https://huggingface.co/Qwen)
- [Llama2 æ¨¡å‹ç”³è¯·](https://ai.meta.com/llama/)
