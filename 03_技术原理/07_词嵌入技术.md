# 07 è¯åµŒå…¥æŠ€æœ¯

> ä»ç¦»æ•£ç¬¦å·åˆ°è¿ç»­å‘é‡ï¼šç†è§£è¯­è¨€è¡¨ç¤ºçš„åŸºçŸ³

## ğŸ“– ä»€ä¹ˆæ˜¯è¯åµŒå…¥ï¼Ÿ

**è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰**æ˜¯å°†è¯è¯­æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´çš„æŠ€æœ¯ï¼š

```
æ–‡æœ¬: "è‹¹æœ"     â†’     å‘é‡: [0.12, -0.34, 0.56, ...]
æ–‡æœ¬: "é¦™è•‰"     â†’     å‘é‡: [0.15, -0.31, 0.60, ...]
æ–‡æœ¬: "ç”µè„‘"     â†’     å‘é‡: [-0.78, 0.23, -0.45, ...]
```

**æ ¸å¿ƒæ€æƒ³**ï¼šè¯­ä¹‰ç›¸ä¼¼çš„è¯åœ¨å‘é‡ç©ºé—´ä¸­è·ç¦»æ›´è¿‘

---

## ğŸ—ï¸ å‘å±•å†ç¨‹

### 1. ç‹¬çƒ­ç¼–ç ï¼ˆOne-Hotï¼‰

```python
# ç®€å•çš„ç‹¬çƒ­ç¼–ç 
vocab = ["è‹¹æœ", "é¦™è•‰", "ç”µè„‘", "æ‰‹æœº"]
vocab_size = len(vocab)

# ç¼–ç 
def one_hot(word, vocab):
    vector = [0] * len(vocab)
    vector[vocab.index(word)] = 1
    return vector

print(one_hot("è‹¹æœ", vocab))  # [1, 0, 0, 0]
print(one_hot("é¦™è•‰", vocab))  # [0, 1, 0, 0]
```

**é—®é¢˜**ï¼š
- ç»´åº¦çˆ†ç‚¸ï¼ˆè¯æ±‡è¡¨10ä¸‡+ï¼‰
- æ— æ³•è¡¨ç¤ºè¯­ä¹‰ç›¸ä¼¼æ€§
- ç¨€ç–è¡¨ç¤ºï¼Œæ•ˆç‡ä½

### 2. è¯è¢‹æ¨¡å‹ï¼ˆBag of Wordsï¼‰

```python
from collections import Counter

def bow(sentence, vocab):
    words = sentence.split()
    vector = [0] * len(vocab)
    word_count = Counter(words)
    
    for i, word in enumerate(vocab):
        vector[i] = word_count.get(word, 0)
    
    return vector

# ç¤ºä¾‹
vocab = ["æˆ‘", "çˆ±", "å­¦ä¹ ", "æœºå™¨", "å­¦ä¹ "]
sentence = "æˆ‘ çˆ± å­¦ä¹ "
print(bow(sentence, vocab))  # [1, 1, 1, 0, 0]
```

**é—®é¢˜**ï¼šå¿½ç•¥è¯åºï¼Œæ— æ³•æ•æ‰è¯­ä¹‰

### 3. TF-IDF

```python
import math

def tf_idf(word, document, corpus):
    # TF: è¯é¢‘
    tf = document.count(word) / len(document)
    
    # IDF: é€†æ–‡æ¡£é¢‘ç‡
    doc_count = sum(1 for doc in corpus if word in doc)
    idf = math.log(len(corpus) / (doc_count + 1))
    
    return tf * idf
```

**æ”¹è¿›**ï¼šè€ƒè™‘è¯çš„é‡è¦æ€§ï¼Œä½†ä»æ˜¯é™æ€è¡¨ç¤º

---

## ğŸ§  ç¥ç»ç½‘ç»œè¯åµŒå…¥

### Word2Vec

**æ ¸å¿ƒæ€æƒ³**ï¼šåŸºäºä¸Šä¸‹æ–‡çš„åˆ†å¸ƒå¼è¡¨ç¤º

```
è‹¹æœå‡ºç°åœ¨"åƒè‹¹æœ" â†’ è¯­ä¹‰ç›¸å…³
è‹¹æœå‡ºç°åœ¨"è‹¹æœæ‰‹æœº" â†’ è¯­ä¹‰ç›¸å…³
```

#### Skip-gramï¼šä¸­å¿ƒè¯é¢„æµ‹ä¸Šä¸‹æ–‡

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SkipGram(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.target_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, target, context):
        # è·å–è¯å‘é‡
        target_vec = self.target_embedding(target)  # [batch, embed_dim]
        context_vec = self.context_embedding(context)  # [batch, embed_dim]
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarity = torch.sum(target_vec * context_vec, dim=1)
        return torch.sigmoid(similarity)

# è®­ç»ƒæ•°æ®ç¤ºä¾‹
# ("è‹¹æœ", "åƒ") â†’ æ­£æ ·æœ¬
# ("è‹¹æœ", "æ‰‹æœº") â†’ æ­£æ ·æœ¬  
# ("è‹¹æœ", "å­¦ä¹ ") â†’ è´Ÿæ ·æœ¬
```

#### CBOWï¼šä¸Šä¸‹æ–‡é¢„æµ‹ä¸­å¿ƒè¯

```
[æˆ‘, åƒ, ä¸€ä¸ª, ] â†’ è‹¹æœ
```

```python
class CBOW(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super().__init__()
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.linear = nn.Linear(embedding_dim, vocab_size)
    
    def forward(self, context_words):
        # ä¸Šä¸‹æ–‡è¯çš„å‘é‡å¹³å‡
        embeds = self.embeddings(context_words)  # [batch, window_size, embed_dim]
        avg_embed = torch.mean(embeds, dim=1)
        
        # é¢„æµ‹ä¸­å¿ƒè¯
        out = self.linear(avg_embed)
        return out
```

### GloVe

**å…¨å±€è¯å‘é‡**ï¼ˆGlobal Vectorsï¼‰

```python
# GloVe æ ¸å¿ƒæ€æƒ³ï¼šå…±ç°çŸ©é˜µ + è¯å‘é‡
# ç›®æ ‡ï¼šä½¿ dot(w_i, w_j) + b_i + b_j â‰ˆ log(X_ij)
# X_ij: è¯ i å’Œè¯ j çš„å…±ç°æ¬¡æ•°

class GloVeLoss(nn.Module):
    def __init__(self, vocab_size, embedding_dim, x_max=100):
        super().__init__()
        self.target_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)
        self.target_bias = nn.Embedding(vocab_size, 1)
        self.context_bias = nn.Embedding(vocab_size, 1)
        
        # æƒé‡å‡½æ•°
        self.x_max = x_max
        self.alpha = 0.75
    
    def weighting(self, x):
        """f(x) æƒé‡å‡½æ•°"""
        return torch.where(
            x < self.x_max,
            (x / self.x_max) ** self.alpha,
            torch.ones_like(x)
        )
    
    def forward(self, target, context, x_ij):
        # è·å–å‘é‡å’Œåç½®
        w_t = self.target_embedding(target)
        w_c = self.context_embedding(context)
        b_t = self.target_bias(target)
        b_c = self.context_bias(context)
        
        # è®¡ç®—æŸå¤±
        weight = self.weighting(x_ij)
        loss = weight * (torch.sum(w_t * w_c, dim=1) + b_t.squeeze() + b_c.squeeze() - torch.log(x_ij)) ** 2
        
        return loss.mean()
```

---

## ğŸ”¢ Transformer ä¸­çš„åµŒå…¥

### è¯è¡¨ä¸åµŒå…¥å±‚

```python
import torch
import torch.nn as nn

class Embeddings(nn.Module):
    def __init__(self, vocab_size, d_model):
        super().__init__()
        self.d_model = d_model
        self.embedding = nn.Embedding(vocab_size, d_model)
    
    def forward(self, x):
        # ä¹˜ä»¥ sqrt(d_model) ç¼©æ”¾
        return self.embedding(x) * math.sqrt(self.d_model)

# ç¤ºä¾‹
vocab_size = 50000
d_model = 512

embeddings = Embeddings(vocab_size, d_model)
input_ids = torch.tensor([[1, 2, 3, 4, 5]])  # batch_size=1, seq_len=5

output = embeddings(input_ids)
print(output.shape)  # [1, 5, 512]
```

### å¯å­¦ä¹ çš„åµŒå…¥

```python
# åœ¨ BERT ä¸­
class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size)
    
    def forward(self, input_ids, token_type_ids=None, position_ids=None):
        seq_length = input_ids.size(1)
        
        # è¯åµŒå…¥
        word_embeds = self.word_embeddings(input_ids)
        
        # ä½ç½®åµŒå…¥
        if position_ids is None:
            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)
            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)
        position_embeds = self.position_embeddings(position_ids)
        
        # å¥å­ç±»å‹åµŒå…¥
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)
        token_type_embeds = self.token_type_embeddings(token_type_ids)
        
        # ç»„åˆ + LayerNorm
        embeddings = word_embeds + position_embeds + token_type_embeds
        embeddings = self.LayerNorm(embeddings)
        
        return embeddings
```

---

## ğŸ“Š åµŒå…¥ç©ºé—´çš„ç‰¹æ€§

### è¯­ä¹‰å…³ç³»

```
å‘é‡è¿ç®—ï¼š
å›½ç‹ - ç”·äºº + å¥³äºº â‰ˆ å¥³ç‹
å·´é» - æ³•å›½ + æ—¥æœ¬ â‰ˆ ä¸œäº¬
```

```python
# ä½¿ç”¨é¢„è®­ç»ƒè¯å‘é‡è¿›è¡Œè¿ç®—
import numpy as np

# å‡è®¾å·²æœ‰è¯å‘é‡
king = np.array([0.5, 0.3, ...])
man = np.array([0.4, 0.2, ...])
woman = np.array([0.1, 0.8, ...])

# è®¡ç®—
result = king - man + woman
# result â‰ˆ queen
```

### å¯è§†åŒ–ï¼ˆt-SNEï¼‰

```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def visualize_embeddings(embeddings, words, labels):
    # t-SNE é™ç»´
    tsne = TSNE(n_components=2, random_state=42)
    embed_2d = tsne.fit_transform(embeddings)
    
    # ç»˜å›¾
    plt.figure(figsize=(10, 10))
    plt.scatter(embed_2d[:, 0], embed_2d[:, 1])
    
    for i, word in enumerate(words):
        plt.annotate(word, (embed_2d[i, 0], embed_2d[i, 1]))
    
    plt.show()
```

---

## ğŸ› ï¸ å®æˆ˜ï¼šä½¿ç”¨é¢„è®­ç»ƒåµŒå…¥

### Word2Vec é¢„è®­ç»ƒæ¨¡å‹

```python
from gensim.models import KeyedVectors

# åŠ è½½ Google News Word2Vec
model = KeyedVectors.load_word2vec_format(
    'GoogleNews-vectors-negative300.bin', 
    binary=True
)

# è·å–è¯å‘é‡
vector = model['apple']
print(f"ç»´åº¦: {len(vector)}")  # 300
print(f"å‘é‡: {vector[:10]}")

# ç›¸ä¼¼è¯
similar = model.most_similar('apple', topn=5)
print(similar)
```

### BERT åµŒå…¥

```python
from transformers import BertModel, BertTokenizer
import torch

# åŠ è½½
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertModel.from_pretrained('bert-base-chinese')

# è¾“å…¥
text = "æˆ‘çˆ±å­¦ä¹ äººå·¥æ™ºèƒ½"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)

# è·å–åµŒå…¥
with torch.no_grad():
    outputs = model(**inputs)
    
# è¯çº§åˆ«åµŒå…¥
word_embeddings = outputs.last_hidden_state  # [batch, seq_len, hidden_size]
print(f"åµŒå…¥ç»´åº¦: {word_embeddings.shape}")  # [1, 9, 768]

# å¥å­çº§åˆ«åµŒå…¥ï¼ˆ[CLS]ï¼‰
sentence_embedding = outputs.last_hidden_state[:, 0, :]
print(f"å¥å­åµŒå…¥ç»´åº¦: {sentence_embedding.shape}")  # [1, 768]
```

---

## ğŸ“ˆ é™æ€åµŒå…¥ vs åŠ¨æ€åµŒå…¥

| ç‰¹æ€§ | é™æ€åµŒå…¥ | åŠ¨æ€åµŒå…¥ |
|------|----------|----------|
| è¡¨ç¤º | å›ºå®šå‘é‡ | ä¸Šä¸‹æ–‡ç›¸å…³ |
| ç¤ºä¾‹ | Word2Vec, GloVe | BERT, GPT |
| å¤šä¹‰è¯ | éš¾ä»¥å¤„ç† | è‡ªåŠ¨åŒºåˆ† |
| è®¡ç®— | ç®€å• | è¾ƒå¤æ‚ |

### å¤šä¹‰è¯ç¤ºä¾‹

```
é™æ€ï¼š"é“¶è¡Œ" åªæœ‰ä¸€ä¸ªå‘é‡

åŠ¨æ€ï¼ˆBERTï¼‰ï¼š
- "é“¶è¡Œå­˜é’±" â†’ é‡‘èæœºæ„
- "æ²³é“¶è¡Œèµ°" â†’ æ²³è¾¹
```

```python
# BERT åŠ¨æ€åµŒå…¥ç¤ºä¾‹
from transformers import BertModel, BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')

sentences = [
    "I went to the bank to deposit money",
    "I sat by the river bank"
]

for sentence in sentences:
    inputs = tokenizer(sentence, return_tensors="pt")
    outputs = model(**inputs)
    
    # è·å–"bank"çš„åµŒå…¥
    bank_token_id = tokenizer.encode("bank", add_special_tokens=False)[0]
    bank_embedding = outputs.last_hidden_state[0, 1, :].numpy()  # ä½ç½®1æ˜¯bank
    
    print(f"å¥å­: {sentence}")
    print(f"bankåµŒå…¥: {bank_embedding[:5]}...")
    print()
```

---

## ğŸ¯ å°ç»“

1. **è¯åµŒå…¥**æ˜¯å°†è¯æ˜ å°„åˆ°ç¨ å¯†å‘é‡ç©ºé—´çš„æŠ€æœ¯
2. **å‘å±•å†ç¨‹**ï¼šç‹¬çƒ­ â†’ è¯è¢‹ â†’ TF-IDF â†’ Word2Vec/GloVe â†’ TransformeråµŒå…¥
3. **è¯­ä¹‰ç‰¹æ€§**ï¼šè¯åµŒå…¥ç©ºé—´å¯ä»¥è¿›è¡Œå‘é‡è¿ç®—è¡¨è¾¾è¯­ä¹‰å…³ç³»
4. **åŠ¨æ€åµŒå…¥**ï¼šä¸Šä¸‹æ–‡ç›¸å…³çš„åµŒå…¥å¯ä»¥å¤„ç†å¤šä¹‰è¯

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781) - Word2Vec åŸå§‹è®ºæ–‡
- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove-paper.pdf) - GloVe è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer è®ºæ–‡

---

*ğŸ”œ ä¸‹ä¸€ç« ï¼š[08_RAGæŠ€æœ¯](../08_RAGæŠ€æœ¯/01_RAGåŸºç¡€æ¦‚å¿µ.md)*

*ğŸ“’ ä¸Šä¸€ç« ï¼š[06_æ¨¡å‹ç»“æ„å¯¹æ¯”](./06_æ¨¡å‹ç»“æ„å¯¹æ¯”.md)*
