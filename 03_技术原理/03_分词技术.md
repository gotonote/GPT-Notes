# 03 åˆ†è¯æŠ€æœ¯

> ç†è§£æ–‡æœ¬å¦‚ä½•è¢«"åˆ‡ç¢"æˆæ¨¡å‹å¯å¤„ç†çš„ tokens

## ğŸ“– ä¸ºä»€ä¹ˆéœ€è¦åˆ†è¯ï¼Ÿ

è®¡ç®—æœºæ— æ³•ç›´æ¥å¤„ç†"æ–‡å­—"ï¼Œåªèƒ½å¤„ç†**æ•°å­—**ã€‚

åˆ†è¯ï¼ˆTokenizationï¼‰å°±æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ—çš„è¿‡ç¨‹ï¼š

```
"ä»Šå¤©å¤©æ°”çœŸå¥½" â†’ [105, 234, 189, 456, 123, 789]
```

---

## ğŸ”¬ åˆ†è¯æ–¹æ³•æ¼”è¿›

```
å­—ç¬¦çº§ â†’ è¯çº§ â†’ BPE â†’ WordPiece â†’ SentencePiece
```

### 1. å­—ç¬¦çº§åˆ†è¯ï¼ˆCharacter-levelï¼‰

å°†æ¯ä¸ªå­—ç¬¦ä½œä¸º tokenã€‚

```python
text = "ä»Šå¤©å¤©æ°”çœŸå¥½"
tokens = list(text)
print(tokens)
# ['ä»Š', 'å¤©', 'å¤©', 'æ°”', 'çœŸ', 'å¥½']
```

**ä¼˜ç‚¹**ï¼šç®€å•ã€vocab å¾ˆå°  
**ç¼ºç‚¹**ï¼šåºåˆ—å¤ªé•¿ã€ä¸¢å¤±è¯­ä¹‰

### 2. è¯çº§åˆ†è¯ï¼ˆWord-levelï¼‰

æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²è¯ã€‚

```python
text = "I love natural language processing"
tokens = text.split()
print(tokens)
# ['I', 'love', 'natural', 'language', 'processing']
```

**é—®é¢˜**ï¼šOOVï¼ˆæœªç™»å½•è¯ï¼‰é—®é¢˜

### 3. BPEï¼ˆByte Pair Encodingï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆå¹¶é«˜é¢‘å­—èŠ‚å¯¹

```python
# æ¨¡æ‹Ÿ BPE è¿‡ç¨‹
vocab = ['a', 'b', 'c', ...]  # åˆå§‹ vocab

def bpe_train(text, num_merges=100):
    """BPE è®­ç»ƒæ¨¡æ‹Ÿ"""
    # 1. æŒ‰å­—ç¬¦åˆ†å‰²
    words = [list(word) for word in text.split()]
    
    # 2. ç»Ÿè®¡ç›¸é‚»å­—ç¬¦å¯¹é¢‘ç‡
    pairs = {}
    for word in words:
        for i in range(len(word) - 1):
            pair = (word[i], word[i+1])
            pairs[pair] = pairs.get(pair, 0) + 1
    
    # 3. åˆå¹¶æœ€é«˜é¢‘çš„å­—ç¬¦å¯¹
    # é‡å¤ç›´åˆ°è¾¾åˆ°ç›®æ ‡ vocab å¤§å°
    return pairs

# ç¤ºä¾‹
text = "low lower lowest"
print(bpe_train(text))
# {('l', 'o'): 3, ('o', 'w'): 3, ('l', 'o', 'w'): 1, ...}
```

### 4. WordPiece

Google BERT ä½¿ç”¨ï¼Œç±»ä¼¼äº BPEï¼Œä½†åŸºäº**æ¦‚ç‡**é€‰æ‹©åˆå¹¶ï¼š

```
Score = (freq_of_pair) / (freq_of_first Ã— freq_of_second)
```

### 5. SentencePiece

- è¯­è¨€æ— å…³
- ç›´æ¥åœ¨åŸå§‹æ–‡æœ¬ä¸Šè®­ç»ƒ
- æ”¯æŒ subword + å­—ç¬¦æ··åˆ

---

## ğŸ› ï¸ å®æˆ˜ï¼šä½¿ç”¨ HuggingFace Tokenizers

```python
from transformers import AutoTokenizer

# åŠ è½½ BERT åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")

text = "ä»Šå¤©å¤©æ°”çœŸå¥½ï¼Œé€‚åˆå‡ºå»æ¸¸ç©"

# åŸºæœ¬åˆ†è¯
tokens = tokenizer.tokenize(text)
print("Tokens:", tokens)
# ['ä»Š', 'å¤©', 'å¤©', 'æ°”', 'çœŸ', 'å¥½', 'ï¼Œ', 'é€‚', 'åˆ', 'å‡º', 'å»', 'æ¸¸', 'ç©']

# ç¼–ç ä¸º ID
input_ids = tokenizer.encode(text)
print("Input IDs:", input_ids)
# [101, 791, 1921, 791, 1921, 3699, 4696, 1962, 8024, 1368, 1457, 770, 7218, 1963, 102]

# è§£ç 
decoded = tokenizer.decode(input_ids)
print("Decoded:", decoded)
```

---

## ğŸ”¢ ç‰¹æ®Š Token

| Token | å«ä¹‰ | ä½œç”¨ |
|-------|------|------|
| `[PAD]` | å¡«å…… | è¡¥é½åºåˆ—é•¿åº¦ |
| `[UNK]` | æœªçŸ¥ | è¯æ±‡è¡¨å¤–çš„è¯ |
| `[CLS]` | åˆ†ç±» | å¥å­å¼€å§‹/åˆ†ç±»ä»»åŠ¡ |
| `[SEP]` | åˆ†éš” | åˆ†éš”å¥å­/å¥å­å¯¹ |
| `[MASK]` | æ©ç  | æ©ç é¢„æµ‹ä»»åŠ¡ |

```python
# æŸ¥çœ‹è¯æ±‡è¡¨
print(f"Vocab size: {tokenizer.vocab_size}")
print(f"Special tokens: {tokenizer.special_tokens_map}")
```

---

## ğŸ“Š ä¸åŒæ¨¡å‹çš„ Vocab Size

| æ¨¡å‹ | åˆ†è¯æ–¹æ³• | Vocab Size |
|------|----------|-------------|
| BERT-base | WordPiece | 30,522 |
| BERT-wwm | WordPiece | 21,128 |
| GPT-2 | BPE | 50,257 |
| LLaMA | SentencePiece | 32,000 |
| ChatGLM | BPE | 64,000 |

---

## ğŸ’¡ å®æˆ˜ï¼šåˆ†æåˆ†è¯æ•ˆæœ

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

texts = [
    "machine learning",
    "æœºå™¨å­¦ä¹ ",
    "ğŸ¦„ğŸ¦„ğŸ¦„"  # emoji
]

for text in texts:
    tokens = tokenizer.tokenize(text)
    ids = tokenizer.encode(text)
    print(f"Text: {text}")
    print(f"  Tokens: {tokens}")
    print(f"  IDs: {ids}")
    print()
```

è¾“å‡ºï¼š
```
Text: machine learning
  Tokens: ['machine', 'Ä learning']
  IDs: [30983, 18497]

Text: æœºå™¨å­¦ä¹ 
  Tokens: ['Ã§', 'Â¥', 'Ã¦', ...]  # UTF-8 å­—èŠ‚

Text: ğŸ¦„ğŸ¦„ğŸ¦„
  Tokens: ['Ã°', 'Â¦', 'Â¬', 'ğŸ¦„', 'Ã°', 'Â¦', 'Â¬', 'ğŸ¦„', ...]
```

---

## âš ï¸ åˆ†è¯å¸¸è§é—®é¢˜

### 1. ä¸­æ–‡åˆ†è¯æŒ‘æˆ˜

```python
# åŒä¸€ä¸ªè¯ï¼Œä¸åŒåˆ†è¯æ–¹å¼
text = "ç ”ç©¶ç”Ÿç‰©"

# æ–¹å¼1: ç ”ç©¶/ç”Ÿç‰©
# æ–¹å¼2: ç ”ç©¶ç”Ÿ/ç‰©

# å½±å“æ¨¡å‹ç†è§£
```

### 2. é•¿åº¦æˆªæ–­

```python
# æˆªæ–­è¿‡é•¿åºåˆ—
max_length = 512

inputs = tokenizer(
    text, 
    max_length=max_length, 
    truncation=True,
    padding='max_length'
)
```

### 3. æ³¨æ„åŠ›æ©ç 

```python
# åŒºåˆ†çœŸå® token å’Œ padding
attention_mask = [1 if ids != tokenizer.pad_token_id else 0 
                   for ids in inputs['input_ids']]
```

---

## ğŸ¯ å°ç»“

1. **åˆ†è¯**æ˜¯å°†æ–‡æœ¬è½¬ä¸ºæ•°å­—çš„å…³é”®æ­¥éª¤
2. **BPE/WordPiece/SentencePiece** æ˜¯ä¸»æµæ–¹æ³•
3. å­è¯åˆ†è¯å¯ä»¥å¹³è¡¡ OOV é—®é¢˜
4. ç‰¹æ®Š token ç”¨äºè¡¨ç¤ºå¥å­è¾¹ç•Œã€åˆ†ç±»ç­‰
5. ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [SentencePiece](https://github.com/google/sentencepiece) - å®˜æ–¹å®ç°
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) - BPE è®ºæ–‡

---

*ğŸ”œ ä¸‹ä¸€ç« ï¼š[04_ä½ç½®ç¼–ç ](./04_ä½ç½®ç¼–ç .md)*
