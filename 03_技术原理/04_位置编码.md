# 04 ä½ç½®ç¼–ç 

> è®©æ¨¡å‹ç†è§£è¯çš„é¡ºåºå…³ç³»

## ğŸ“– ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

**Transformer ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼Œè€Œè‡ªæ³¨æ„åŠ›æœ¬èº«æ˜¯**ä½ç½®æ— å…³**çš„ï¼š

```
"ç‹—å’¬äºº" vs "äººå’¬ç‹—"
â†’ è‡ªæ³¨æ„åŠ›æ— æ³•åŒºåˆ†é¡ºåºï¼
```

ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰å°±æ˜¯ä¸ºè§£å†³è¿™ä¸€é—®é¢˜è€Œè®¾è®¡çš„ã€‚

---

## ğŸ”¬ ä½ç½®ç¼–ç æ–¹æ³•

### 1. ç»å¯¹ä½ç½®ç¼–ç ï¼ˆAbsolute Positional Encodingï¼‰

#### 1.1 ä¸‰è§’å‡½æ•°ç¼–ç ï¼ˆSinusoidalï¼‰

Transformer åŸå§‹è®ºæ–‡ä½¿ç”¨ï¼š

```
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

| å‚æ•° | å«ä¹‰ |
|------|------|
| pos | ä½ç½®ï¼ˆ0, 1, 2, ...ï¼‰ |
| i | ç»´åº¦ç´¢å¼• |
| d_model | æ¨¡å‹éšè—ç»´åº¦ |

```python
import torch
import math

class PositionalEncoding(torch.nn.Module):
    """Sinusoidal ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        # è®¡ç®—é™¤æ•°
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * 
            (-math.log(10000.0) / d_model)
        )
        
        # å¶æ•°ç»´åº¦ï¼šsin
        pe[:, 0::2] = torch.sin(position * div_term)
        # å¥‡æ•°ç»´åº¦ï¼šcos
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # æ·»åŠ  batch ç»´åº¦
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """æ·»åŠ ä½ç½®ç¼–ç åˆ°è¾“å…¥"""
        # x: [batch, seq_len, d_model]
        return x + self.pe[:, :x.size(1)]


# ä½¿ç”¨ç¤ºä¾‹
d_model = 512
seq_len = 10
pos_encoder = PositionalEncoding(d_model)

# éšæœºè¾“å…¥
x = torch.randn(1, seq_len, d_model)
x_encoded = pos_encoder(x)

print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"ç¼–ç åå½¢çŠ¶: {x_encoded.shape}")
```

#### 1.2 å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼ˆLearnableï¼‰

```python
class LearnablePositionalEncoding(torch.nn.Module):
    """å¯å­¦ä¹ çš„ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        self.pos_embedding = nn.Embedding(max_len, d_model)
    
    def forward(self, x):
        # x: [batch, seq_len, d_model]
        batch_size, seq_len, _ = x.size()
        
        # åˆ›å»ºä½ç½®ç´¢å¼• [0, 1, 2, ..., seq_len-1]
        positions = torch.arange(seq_len).to(x.device)
        
        # è·å–ä½ç½®åµŒå…¥å¹¶ç›¸åŠ 
        pos_embed = self.pos_embedding(positions)
        return x + pos_embed.unsqueeze(0)
```

---

### 2. ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆRelative Positional Encodingï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šç¼–ç **ç›¸å¯¹ä½ç½®**è€Œéç»å¯¹ä½ç½®

```python
class RelativePositionBias(nn.Module):
    """ç›¸å¯¹ä½ç½®åç½®ï¼ˆç”¨äº T5 ç­‰æ¨¡å‹ï¼‰"""
    
    def __init__(self, num_heads, max_distance=128):
        super().__init__()
        self.num_heads = num_heads
        self.max_distance = max_distance
        
        # ç›¸å¯¹ä½ç½®åç½®è¡¨
        self.relative_attention_bias = nn.Embedding(
            2 * max_distance + 1, 
            num_heads
        )
    
    def forward(self, seq_len):
        # ç”Ÿæˆç›¸å¯¹ä½ç½®ç´¢å¼•
        # positions: [0, 1, 2, ..., seq_len-1]
        positions = torch.arange(seq_len)
        # ç›¸å¯¹ä½ç½®çŸ©é˜µ: [i - j for all i, j]
        relative_positions = positions.unsqueeze(1) - positions.unsqueeze(0)
        
        # é™åˆ¶èŒƒå›´ [-max_distance, max_distance]
        relative_positions = torch.clamp(
            relative_positions, 
            -self.max_distance, 
            self.max_distance
        )
        
        # åç§»ï¼Œä½¿ç´¢å¼•ä» 0 å¼€å§‹
        relative_positions += self.max_distance
        
        # è·å–åç½®
        return self.relative_attention_bias(relative_positions)
```

---

## ğŸ“Š ç¼–ç å¯¹æ¯”

| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | ä»£è¡¨æ¨¡å‹ |
|------|------|------|----------|
| Sinusoidal | å¯å¤–æ¨ã€æ˜¾å¼å‘¨æœŸ | éš¾ä»¥å­¦ä¹  | Transformer |
| Learnable | å¯è®­ç»ƒ | é•¿åº¦å›ºå®š | BERT |
| Relative | æ•è·ç›¸å¯¹å…³ç³» | å®ç°å¤æ‚ | T5, XLNet |
| RoPE | å¯å¤–æ¨ã€æ— æ˜¾å¼åç½® | åªèƒ½ç”¨äºæ³¨æ„åŠ› | LLaMA, ChatGLM |

---

## ğŸŒ€ RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰

LLaMAã€ChatGLM ç­‰æ¨¡å‹ä½¿ç”¨çš„**æ—‹è½¬ä½ç½®ç¼–ç **ï¼š

```python
import numpy as np

def rotate_half(x):
    """æ—‹è½¬åŠä¸ªå‘é‡"""
    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
    return torch.cat([-x2, x1], dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    # q, k: [batch, heads, seq_len, head_dim]
    # cos, sin: [seq_len, head_dim]
    q_embed = q * cos + rotate_half(q) * sin
    k_embed = k * cos + rotate_half(k) * sin
    return q_embed, k_embed


class RoPE(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç """
    
    def __init__(self, dim, max_seq_len=2048):
        super().__init__()
        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
        # é¢„è®¡ç®— cos, sin
        t = torch.arange(max_seq_len).unsqueeze(1) * inv_freq.unsqueeze(0)
        emb = torch.cat([t, t], dim=-1)
        cos = emb.cos()
        sin = emb.sin()
        self.register_buffer('cos_cached', cos)
        self.register_buffer('sin_cached', sin)
    
    def forward(self, x, seq_len=None):
        # x: [batch, n_heads, seq_len, head_dim]
        if seq_len is None:
            seq_len = x.size(2)
        return (
            self.cos_cached[:seq_len],
            self.sin_cached[:seq_len]
        )
```

---

## ğŸ’¡ å¯è§†åŒ–ï¼šä½ç½®ç¼–ç æ¨¡å¼

```python
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ç”Ÿæˆ Sinusoidal ä½ç½®ç¼–ç 
d_model = 64
max_len = 100

positions = np.arange(max_len).reshape(-1, 1)
i = np.arange(0, d_model, 2)
div_term = np.exp(-np.log(10000) * i / d_model)

pe = np.zeros((max_len, d_model))
pe[:, 0::2] = np.sin(positions * div_term)
pe[:, 1::2] = np.cos(positions * div_term)

# å¯è§†åŒ–
plt.figure(figsize=(12, 6))
sns.heatmap(pe[:20, :32], cmap='RdBu_r', center=0)
plt.xlabel('Dimension')
plt.ylabel('Position')
plt.title('Sinusoidal Positional Encoding (first 20 positions)')
plt.show()
```

---

## ğŸ¯ å°ç»“

1. **ä½ç½®ç¼–ç **è®© Transformer æ„ŸçŸ¥åºåˆ—é¡ºåº
2. **Sinusoidal** ä½¿ç”¨ä¸‰è§’å‡½æ•°ï¼Œæ˜¾å¼å‘¨æœŸæ€§
3. **Learnable** å¯è®­ç»ƒçš„åµŒå…¥
4. **Relative** ç¼–ç ç›¸å¯¹ä½ç½®å…³ç³»
5. **RoPE** æ—‹è½¬ç¼–ç ï¼Œå¯æ‰©å±•æ€§å¼º

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - åŸå§‹è®ºæ–‡
- [RoFormer](https://arxiv.org/abs/2104.09864) - RoPE è®ºæ–‡

---

*ğŸ”œ ä¸‹ä¸€ç« ï¼š[05_å¤§æ¨¡å‹å·¥ä½œåŸç†](./05_å¤§æ¨¡å‹å·¥ä½œåŸç†.md)*
