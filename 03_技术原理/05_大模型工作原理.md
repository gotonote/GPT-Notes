# 05 å¤§æ¨¡å‹å·¥ä½œåŸç†

> æ·±å…¥ç†è§£ LLM å¦‚ä½•"å­¦ä¼š"è¯´è¯

## ğŸ“– æ•´ä½“æµç¨‹

å¤§è¯­è¨€æ¨¡å‹çš„å·¥ä½œæµç¨‹ï¼š

```
æ–‡æœ¬è¾“å…¥ â†’ åˆ†è¯(Tokenization) â†’ åµŒå…¥(Embedding) â†’ ç¥ç»ç½‘ç»œ â†’ é¢„æµ‹ â†’ è§£ç  â†’ æ–‡æœ¬è¾“å‡º
```

---

## ğŸ”„ è®­ç»ƒèŒƒå¼

### 1. é¢„è®­ç»ƒï¼ˆPretrainingï¼‰

**ç›®æ ‡**ï¼šå­¦ä¹ é€šç”¨è¯­è¨€è¡¨ç¤º

```python
# ç®€åŒ–çš„é¢„è®­ç»ƒè¿‡ç¨‹
def pretrain(model, dataloader, optimizer):
    model.train()
    total_loss = 0
    
    for batch in dataloader:
        # è·å–è¾“å…¥
        input_ids = batch['input_ids']
        labels = batch['labels']
        
        # å‰å‘ä¼ æ’­
        outputs = model(input_ids)
        loss = F.cross_entropy(outputs.view(-1, vocab_size), labels.view(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)
```

### 2. æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰

**ç›®æ ‡**ï¼šè®©æ¨¡å‹ç†è§£æŒ‡ä»¤

```
è¾“å…¥: "æŠŠä»¥ä¸‹å¥å­ç¿»è¯‘æˆè‹±æ–‡ï¼šæˆ‘çˆ±å­¦ä¹ "
è¾“å‡º: "I love studying"
```

### 3. å¯¹é½å¾®è°ƒï¼ˆRLHFï¼‰

**ç›®æ ‡**ï¼šè®©æ¨¡å‹è¾“å‡ºç¬¦åˆäººç±»åå¥½

```
å¥–åŠ±æ¨¡å‹: åˆ¤æ–­è¾“å‡ºè´¨é‡
PPO: ä¼˜åŒ–ç­–ç•¥
```

---

## ğŸ§® è‡ªå›å½’ç”Ÿæˆ

å¤§è¯­è¨€æ¨¡å‹ï¼ˆGPT ç³»åˆ—ï¼‰æ˜¯**è‡ªå›å½’æ¨¡å‹**ï¼š

```
P(x) = P(xâ‚) Ã— P(xâ‚‚|xâ‚) Ã— P(xâ‚ƒ|xâ‚,xâ‚‚) Ã— ... Ã— P(xâ‚™|xâ‚,...,xâ‚™â‚‹â‚)
```

### ç”Ÿæˆè¿‡ç¨‹

```python
def generate(model, tokenizer, prompt, max_length=100):
    """è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆ"""
    
    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    
    # ç”Ÿæˆ
    model.eval()
    with torch.no_grad():
        for _ in range(max_length):
            # å‰å‘ä¼ æ’­
            outputs = model(input_ids)
            logits = outputs.logits[:, -1, :]  # åªå–æœ€åä¸€ä¸ªä½ç½®
            
            # é‡‡æ ·ç­–ç•¥
            # æ–¹æ³•1: è´ªå¿ƒ (Greedy)
            # next_token = logits.argmax(dim=-1)
            
            # æ–¹æ³•2: æ¸©åº¦é‡‡æ · (Temperature)
            # logits = logits / temperature
            # next_token = F.softmax(logits, dim=-1).multinomial(1)
            
            # æ–¹æ³•3: Top-K é‡‡æ ·
            # top_k = 50
            # indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1]
            # logits[indices_to_remove] = float('-inf')
            
            # æ–¹æ³•4: Top-P (Nucleus) é‡‡æ ·
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumsum = F.softmax(sorted_logits, dim=-1).cumsum(dim=-1)
            indices_to_remove = cumsum > top_p
            indices_to_remove[..., 1:] = indices_to_remove[..., :-1].clone()
            indices_to_remove[..., 0] = False
            logits[torch.gather(sorted_indices, -1, indices_to_remove)] = float('-inf')
            
            # é€‰æ‹©ä¸‹ä¸€ä¸ª token
            probs = F.softmax(logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # ç»ˆæ­¢æ¡ä»¶
            if next_token == tokenizer.eos_token_id:
                break
            
            # è¿½åŠ åˆ°è¾“å…¥
            input_ids = torch.cat([input_ids, next_token], dim=-1)
    
    return tokenizer.decode(input_ids[0])
```

---

## ğŸ“Š å¸¸ç”¨é‡‡æ ·ç­–ç•¥

| ç­–ç•¥ | æè¿° | ç‰¹ç‚¹ | æ•ˆæœ |
|------|------|------|------|
| **Greedy** | æ€»æ˜¯é€‰æœ€é«˜æ¦‚ç‡ | ç¡®å®šæ€§å¼º | å®¹æ˜“é‡å¤ |
| **Temperature** | è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒ | æ§åˆ¶éšæœºæ€§ | tâ†‘ éšæœº, tâ†“ ç¡®å®š |
| **Top-K** | ä»å‰ K ä¸ªä¸­é‡‡æ · | é™åˆ¶èŒƒå›´ | ç®€å•æœ‰æ•ˆ |
| **Top-P** | ä»ç´¯è®¡æ¦‚ç‡é˜ˆå€¼é‡‡æ · | åŠ¨æ€èŒƒå›´ | æ›´çµæ´» |

### ä»£ç ç¤ºä¾‹

```python
import torch
import torch.nn.functional as F

def sample_with_strategy(logits, strategy='greedy', **kwargs):
    """ä¸åŒçš„é‡‡æ ·ç­–ç•¥"""
    
    if strategy == 'greedy':
        return torch.argmax(logits, dim=-1)
    
    elif strategy == 'temperature':
        temperature = kwargs.get('temperature', 1.0)
        logits = logits / temperature
        return F.softmax(logits, dim=-1).multinomial(1)
    
    elif strategy == 'top_k':
        top_k = kwargs.get('top_k', 50)
        top_k = min(top_k, logits.size(-1))
        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]
        logits[indices_to_remove] = float('-inf')
        return F.softmax(logits, dim=-1).multinomial(1)
    
    elif strategy == 'top_p':
        top_p = kwargs.get('top_p', 0.9)
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        probs = F.softmax(sorted_logits, dim=-1)
        cumsum = probs.cumsum(dim=-1)
        sorted_indices_to_remove = cumsum > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = float('-inf')
        return F.softmax(logits, dim=-1).multinomial(1)
```

---

## ğŸ¯ ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-Context Learningï¼‰

LLM çš„ç¥å¥‡èƒ½åŠ›ï¼š**æ— éœ€æ¢¯åº¦æ›´æ–°ï¼Œä»…é€šè¿‡æç¤ºå°±èƒ½å­¦ä¹ **

### Few-Shot ç¤ºä¾‹

```
ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†ç±»

ç¤ºä¾‹ï¼š
æ–‡æœ¬: "è¿™éƒ¨ç”µå½±å¤ªç²¾å½©äº†" â†’ æ­£é¢
æ–‡æœ¬: "æœåŠ¡æ€åº¦å¤ªå·®" â†’ è´Ÿé¢

å¾…åˆ†ç±»: "å‘³é“è¿˜ä¸é”™"
```

æ¨¡å‹æ— éœ€è®­ç»ƒå°±èƒ½ç†è§£ä»»åŠ¡ï¼

### åŸç†å‡è¯´

1. **å…ƒå­¦ä¹ **ï¼šé¢„è®­ç»ƒæ—¶è§è¿‡ç±»ä¼¼æ¨¡å¼
2. **è¯­å¢ƒæ¨¡æ‹Ÿ**ï¼šæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡ç¤ºä¾‹å…³ç³»
3. **éšå¼æ¨ç†**ï¼šå¤§æ¨¡å‹å…·æœ‰æ¨ç†èƒ½åŠ›

---

## ğŸ“ˆ æ¨¡å‹è§„æ¨¡çš„å½±å“

| è§„æ¨¡ | å…¸å‹æ¨¡å‹ | æ¶Œç°èƒ½åŠ› |
|------|----------|----------|
| < 1B | GPT-2 | åŸºæœ¬è¯­è¨€å»ºæ¨¡ |
| 1-10B | GPT-3 small | Few-shot å­¦ä¹  |
| 10-100B | GPT-3 | æ€ç»´é“¾ã€æ¨ç† |
| > 100B | GPT-4 | å¤šæ¨¡æ€ã€å¤æ‚æ¨ç† |

> **æ¶Œç°èƒ½åŠ›**ï¼šæ¨¡å‹è§„æ¨¡è¶…è¿‡ä¸´ç•Œç‚¹åï¼Œçªç„¶å‡ºç°çš„æ–°èƒ½åŠ›

---

## ğŸ’» å®æˆ˜ï¼šä½¿ç”¨ Transformers ç”Ÿæˆæ–‡æœ¬

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# åŠ è½½æ¨¡å‹
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# é…ç½®
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# è¾“å…¥
prompt = "Once upon a time"

# ç¼–ç 
inputs = tokenizer(prompt, return_tensors="pt").to(device)

# ç”Ÿæˆ
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_length=100,
        temperature=0.7,
        top_k=50,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

# è§£ç 
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

---

## ğŸ¯ å°ç»“

1. **è‡ªå›å½’ç”Ÿæˆ**ï¼šé€è¯é¢„æµ‹ä¸‹ä¸€ä¸ª token
2. **é‡‡æ ·ç­–ç•¥**ï¼šæ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§ä¸ç¡®å®šæ€§
3. **æ¶Œç°èƒ½åŠ›**ï¼šå¤§æ¨¡å‹è¶…è¶Šè§„æ¨¡çš„è¡¨ç°
4. **ä¸Šä¸‹æ–‡å­¦ä¹ **ï¼šæ— éœ€è®­ç»ƒå°±èƒ½å­¦ä¹ æ–°ä»»åŠ¡

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3 è®ºæ–‡
- [Chain-of-Thought Prompting](https://arxiv.org/abs/2201.11903) - æ€ç»´é“¾

---

*ğŸ”œ ä¸‹ä¸€ç« ï¼š[06_æ¨¡å‹ç»“æ„å¯¹æ¯”](./06_æ¨¡å‹ç»“æ„å¯¹æ¯”.md)*
