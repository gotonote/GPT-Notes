# 02 注意力机制

> Transformer 的核心，理解模型如何"关注"关键信息

## 📖 什么是注意力机制？

**注意力机制（Attention Mechanism）** 模拟人类选择性视觉注意力的行为：
- 在众多信息中，**聚焦**最重要的部分
- 动态分配**权重**给不同位置
- 捕捉**长距离依赖**关系

### 🎯 直观理解

```
句子: "The animal didn't cross the street because it was too tired"

问题: "it" 指的是什么？
答案: "The animal"

注意力机制帮助模型学习这种指代关系！
```

---

## 🧮 数学原理

### Scaled Dot-Product Attention

Transformer 使用的是 **缩放点积注意力**：

```
Attention(Q, K, V) = softmax( QK^T / √d_k ) * V
```

| 符号 | 含义 |
|------|------|
| **Q (Query)** | 查询向量 - 当前正在处理的位置 |
| **K (Key)** | 键向量 - 所有位置的身份标识 |
| **V (Value)** | 值向量 - 实际的信息内容 |
| **d_k** | 向量维度（用于缩放） |

### 为什么需要缩放？

- 当 d_k 较大时，点积结果会很大
- 会导致 softmax 梯度极小
- 除以 √d_k 可以**稳定训练**

---

## 🔧 PyTorch 实现

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class ScaledDotProductAttention(nn.Module):
    """缩放点积注意力"""
    
    def __init__(self, dropout=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, Q, K, V, attn_mask=None):
        d_k = Q.size(-1)
        
        # 计算点积
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # 掩码操作（可选）
        if attn_mask is not None:
            scores = scores.masked_fill(attn_mask == 0, -1e9)
        
        # softmax 归一化
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 加权求和
        output = torch.matmul(attn_weights, V)
        
        return output, attn_weights


class MultiHeadAttention(nn.Module):
    """多头注意力"""
    
    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()
        assert d_model % n_heads == 0
        
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        # 线性变换
        self.W_Q = nn.Linear(d_model, d_model)
        self.W_K = nn.Linear(d_model, d_model)
        self.W_V = nn.Linear(d_model, d_model)
        self.W_O = nn.Linear(d_model, d_model)
        
        self.attention = ScaledDotProductAttention(dropout)
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, Q, K, V, attn_mask=None):
        batch_size = Q.size(0)
        
        # 1. 线性变换并分头
        Q = self.W_Q(Q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_K(K).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_V(V).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        
        # 2. 计算注意力
        output, attn_weights = self.attention(Q, K, V, attn_mask)
        
        # 3. 拼接多头
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        
        # 4. 最终线性变换
        output = self.W_O(output)
        
        return output, attn_weights
```

---

## 👁️ 可视化：注意力权重

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_attention(attn_weights, tokens):
    """可视化注意力权重"""
    plt.figure(figsize=(10, 8))
    sns.heatmap(attn_weights, 
                xticklabels=tokens, 
                yticklabels=tokens,
                cmap='viridis')
    plt.title('Attention Weights')
    plt.xlabel('Key')
    plt.ylabel('Query')
    plt.show()

# 示例注意力矩阵
attn_matrix = torch.rand(5, 5)  # 5x5 注意力
tokens = ['The', 'animal', 'didn\'t', 'cross', 'street']

visualize_attention(attn_matrix, tokens)
```

---

## 🌍 多头注意力的优势

| 优势 | 说明 |
|------|------|
| **并行捕获多种关系** | 每个头可以学习不同的语义关系 |
| **增强模型表达能力** | 组合多个注意力模式 |
| **可解释性** | 不同头可能关注不同的语法/语义特征 |

### 示例：不同头学习不同模式

```python
# 注意力头可以学习不同模式
heads = {
    '句法关系': '学习主谓宾结构',
    '语义关系': '学习实体关联',
    '位置关系': '学习相邻词关系',
    '指代关系': '学习代词指代'
}
```

---

## 🔄 自注意力 vs 交叉注意力

| 类型 | Q 来源 | K, V 来源 | 应用场景 |
|------|--------|-----------|----------|
| **自注意力** | 同一序列 | 同一序列 | Encoder / 内部建模 |
| **交叉注意力** | Decoder | Encoder | Decoder / 跨序列 |

---

## 📊 复杂度分析

| 操作 | 时间复杂度 | 空间复杂度 |
|------|-----------|-----------|
| RNN | O(n × d²) | O(n × d) |
| CNN | O(n × k × d²) | O(n × d) |
| **自注意力** | O(n² × d) | O(n² × d) |

> 注：n = 序列长度，d = 隐藏层维度，k = 卷积核大小

---

## 🎯 小结

1. **注意力机制** 是 Transformer 的核心
2. 通过 Q、K、V 计算相似度权重
3. **多头注意力** 可以捕获多种语义关系
4. 优点：并行计算、捕获长距离依赖
5. 缺点：序列长度二次方复杂度

---

## 📚 延伸阅读

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - 原始论文
- [Illustrated Self-Attention](https://towardsdatascience.com/illustrated-self-attention-e6d93a27f783) - 图解自注意力

---

*🔜 下一章：[03_分词技术](./03_分词技术.md)*
