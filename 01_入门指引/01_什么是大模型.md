# 第一章：什么是大模型

> 本章将带你了解什么是大语言模型（Large Language Model，LLM），以及人工智能领域的基本术语和概念。

## 1.1 人工智能与机器学习

### 什么是人工智能？

**人工智能（Artificial Intelligence，AI）** 是计算机科学的一个分支，致力于开发能够模拟、延伸和扩展人类智能的理论、方法和技术。AI 的目标是让机器具有感知、推理、学习、决策等能力。

### 什么是机器学习？

**机器学习（Machine Learning，ML）** 是人工智能的一个子领域，它使计算机能够从数据中学习并改进性能，而无需被明确编程。机器学习主要分为三类：

| 类型 | 描述 | 示例 |
|------|------|------|
| **监督学习** | 从标注数据中学习 | 图像分类、垃圾邮件检测 |
| **无监督学习** | 从无标注数据中发现模式 | 聚类、降维 |
| **强化学习** | 通过与环境交互学习最优策略 | 游戏AI、机器人控制 |

### 什么是深度学习？

**深度学习（Deep Learning，DL）** 是机器学习的一个分支，使用多层神经网络（称为深度神经网络）来学习数据的层次化表示。深度学习在图像识别、语音识别和自然语言处理等领域取得了突破性进展。

```
人工智能 (AI)
    └── 机器学习 (ML)
            └── 深度学习 (DL)
                    └── 大语言模型 (LLM)
```

---

## 1.2 什么是大语言模型？

### 定义

**大语言模型（Large Language Model，LLM）** 是一种基于深度学习技术训练的大规模神经网络模型，专门用于处理和生成自然语言文本。

LLM 的核心特征：

- **大规模参数**：通常拥有数十亿到数千亿个参数
- **大规模预训练**：在海量文本数据上进行无监督学习
- **通用能力**：具备语言理解、生成、推理等多样能力
- **涌现能力**：随着模型规模增大，会出现小模型不具备的能力

### LLM 与传统 NLP 的区别

| 特性 | 传统NLP | 大语言模型 |
|------|---------|------------|
| 训练方式 | 针对特定任务 | 无监督预训练 + 任务微调 |
| 数据需求 | 任务相关标注数据 | 海量无标注文本 |
| 模型规模 | 百万级参数 | 十亿/千亿级参数 |
| 泛化能力 | 特定任务 | 通用多任务 |
| 部署成本 | 较低 | 较高 |

### 常见的 LLM 示例

| 模型 | 开发者 | 发布时间 | 参数规模 |
|------|--------|----------|----------|
| GPT-4 | OpenAI | 2023.03 | ~1.7T |
| GPT-3.5 | OpenAI | 2022.11 | 175B |
| Claude 3 | Anthropic | 2024.03 | - |
| Gemini | Google | 2023.12 | - |
| LLaMA 3 | Meta | 2024.04 | 70B |
| Qwen | 阿里巴巴 | 2024 | 72B/110B |
| ChatGLM | 智谱AI | 2024 | 6B/130B |

---

## 1.3 核心术语科普

### 1.3.1 Token（词元）

**Token** 是文本处理的基本单位。在 NLP 中，文本被分割成一个个 token 进行处理。

```python
# Token 示例
text = "你好，世界！"
tokens = ["你", "好", "，", "世", "界", "!"]
# 在实际应用中，可能会使用更细粒度或更粗粒度的分词策略
```

> 💡 **提示**：不同模型使用不同的分词器（Tokenizer），同一个文本可能被分割成不同数量的 token。

### 1.3.2 Embedding（嵌入）

**Embedding（嵌入）** 是将离散的符号（如文字、单词）转换为连续的向量表示的技术。

```python
# 简单的词嵌入示例
# 假设词表大小为 10000，嵌入维度为 768
embedding_layer = nn.Embedding(num_embeddings=10000, embedding_dim=768)

# 将 token id 转换为嵌入向量
token_ids = torch.tensor([1, 2, 3, 4])  # 示例 token ids
embeddings = embedding_layer(token_ids)
# 输出形状: (4, 768)
```

### 1.3.3 Transformer

**Transformer** 是一种基于自注意力机制（Self-Attention）的神经网络架构，是现代大语言模型的基础。

```python
# Transformer 核心组件 - 自注意力机制简化版
import torch
import torch.nn.functional as F

def simplified_attention(Q, K, V):
    """
    简化的自注意力机制
    
    参数:
        Q: Query 查询矩阵 (batch, seq_len, d_k)
        K: Key 键矩阵 (batch, seq_len, d_k)
        V: Value 值矩阵 (batch, seq_len, d_v)
    """
    # 计算注意力分数
    d_k = Q.size(-1)
    scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)
    
    # 使用 softmax 得到注意力权重
    attention_weights = F.softmax(scores, dim=-1)
    
    # 加权求和得到输出
    output = torch.matmul(attention_weights, V)
    
    return output, attention_weights
```

### 1.3.4 参数（Parameters）

**参数** 是模型在训练过程中学习的权重和偏置。模型的"大小"通常用参数数量来衡量。

```
参数数量示例：
- GPT-3: 175 billion (1750亿)
- GPT-4: ~1.7 trillion (1.7万亿)
- LLaMA 3 70B: 700亿
```

### 1.3.5 预训练与微调

| 概念 | 描述 |
|------|------|
| **预训练（Pre-training）** | 在大规模数据上训练基础模型，学习通用语言知识 |
| **微调（Fine-tuning）** | 在特定任务数据上继续训练，使模型适应特定任务 |

### 1.3.6 Prompt（提示词）

**Prompt** 是用户输入给 LLM 的文本指令，用于引导模型生成期望的输出。

```python
# Prompt 示例
prompt = """
你是一位专业的Python编程助手。请解释以下代码的功能：

def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)
"""
```

### 1.3.7 Temperature（温度）

**Temperature** 是控制模型输出随机性的参数。

- **Temperature = 0**：输出最确定，最可能的结果
- **Temperature = 1**：正常采样
- **Temperature > 1**：输出更随机、更有创意

```python
# API 调用时设置 temperature
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "给我讲个笑话"}],
    temperature=0.7  # 控制随机性
)
```

### 1.3.8 Top-k / Top-p（采样策略）

| 参数 | 描述 |
|------|------|
| **Top-k** | 保留概率最高的 k 个 token |
| **Top-p** | 保留累计概率超过 p 的最小 token 集合 |

---

## 1.4 模型能力与局限

### LLM 能做什么？

✅ **语言理解**：阅读理解、情感分析、文本分类
✅ **语言生成**：写作文、写代码、写邮件
✅ **知识问答**：回答各种问题
✅ **代码相关**：代码补全、代码调试、技术文档
✅ **推理能力**：数学题、逻辑推理
✅ **多语言支持**：翻译、跨语言理解

### LLM 的局限性

⚠️ **知识截止**：模型知识有时间限制，可能不知道最新信息
⚠️ **幻觉**：可能生成看似合理但实际错误的内容
⚠️ **推理局限**：复杂推理能力有限
⚠️ **资源消耗**：推理需要大量计算资源
⚠️ **偏见**：可能反映训练数据中的偏见

---

## 1.5 本章小结

本章我们学习了：

1. **人工智能、机器学习、深度学习** 的关系和定义
2. **大语言模型（LLM）** 是什么，以及它与传统 NLP 的区别
3. **核心术语**：Token、Embedding、Transformer、参数、Prompt、Temperature 等
4. **LLM 的能力与局限性**

---

## 📚 延伸阅读

- [Transformer: Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [GPT 系列论文](https://cdn.openai.com/research-covers/language-unsupervised/)

---

*下一章我们将学习大模型的发展历程，了解 ChatGPT 是如何诞生的。*
