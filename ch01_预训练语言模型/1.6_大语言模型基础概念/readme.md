# 1.6 å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLLMï¼‰åŸºç¡€æ¦‚å¿µ

> æœ¬èŠ‚å†…å®¹é¢å‘åˆå­¦è€…ï¼Œè¯¦ç»†ä»‹ç»å¤§è¯­è¨€æ¨¡åž‹çš„åŸºç¡€æ¦‚å¿µå’Œæ ¸å¿ƒæœ¯è¯­

## ä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡åž‹ï¼Ÿ

**å¤§è¯­è¨€æ¨¡åž‹ï¼ˆLarge Language Modelï¼Œç®€ç§° LLMï¼‰** æ˜¯ä¸€ç§ç»è¿‡å¤§è§„æ¨¡æ–‡æœ¬æ•°æ®è®­ç»ƒçš„äººå·¥æ™ºèƒ½æ¨¡åž‹ï¼Œèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚è¿™ç±»æ¨¡åž‹é€šè¿‡å­¦ä¹ æµ·é‡æ–‡æœ¬æ•°æ®ä¸­çš„è¯­è¨€æ¨¡å¼ã€è¯­æ³•ç»“æž„ã€çŸ¥è¯†å¸¸è¯†ç­‰ï¼Œå®žçŽ°äº†å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†èƒ½åŠ›ã€‚

### ç®€å•ç†è§£

ä½ å¯ä»¥æŠŠ LLM æƒ³è±¡æˆä¸€ä¸ª"é˜…è¯»è¿‡å…¨ä¸–ç•Œå¤§éƒ¨åˆ†ä¹¦ç±çš„äºº"ã€‚å®ƒé€šè¿‡é˜…è¯»å¤§é‡çš„æ–‡æœ¬ï¼Œå­¦ä¼šäº†è¯­è¨€çš„è§„å¾‹å’ŒçŸ¥è¯†ï¼Œå› æ­¤èƒ½å¤Ÿï¼š
- å›žç­”å„ç§é—®é¢˜
- å†™æ–‡ç« ã€å†™ä»£ç 
- ç¿»è¯‘è¯­è¨€
- è¿›è¡Œå¯¹è¯äº¤æµ

## LLM çš„æ ¸å¿ƒç‰¹å¾

### 1. å‚æ•°è§„æ¨¡å·¨å¤§

"å¤§"å­—ä¸»è¦ä½“çŽ°åœ¨æ¨¡åž‹çš„å‚æ•°æ•°é‡ä¸Šã€‚å‚æ•°å¯ä»¥ç†è§£ä¸ºæ¨¡åž‹"è„‘ä¸­çš„ç¥žç»è¿žæŽ¥"ï¼Œæ•°é‡è¶Šå¤šï¼Œæ¨¡åž‹çš„èƒ½åŠ›è¶Šå¼ºã€‚

| æ¨¡åž‹çº§åˆ« | å‚æ•°è§„æ¨¡ | ä»£è¡¨æ¨¡åž‹ |
|---------|---------|---------|
| å°åž‹æ¨¡åž‹ | ç™¾ä¸‡çº§ | DistilBERT |
| ä¸­åž‹æ¨¡åž‹ | äº¿çº§ | BERT-Large |
| å¤§åž‹æ¨¡åž‹ | åäº¿çº§ | GPT-2ã€LLaMA-7B |
| è¶…å¤§æ¨¡åž‹ | ç™¾äº¿çº§ | GPT-3ã€ChatGLM-6B |
| å·¨åž‹æ¨¡åž‹ | åƒäº¿çº§ | GPT-4ã€PaLM |

> **æ³¨æ„**ï¼šå‚æ•°è§„æ¨¡åªæ˜¯è¡¡é‡æ¨¡åž‹èƒ½åŠ›çš„æŒ‡æ ‡ä¹‹ä¸€ï¼Œæ•°æ®è´¨é‡ã€è®­ç»ƒæ–¹æ³•åŒæ ·é‡è¦ã€‚

### 2. é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼

LLM çš„è®­ç»ƒé€šå¸¸åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   é¢„è®­ç»ƒé˜¶æ®µ (Pre-training)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  è¾“å…¥ï¼šæµ·é‡æ— æ ‡ç­¾æ–‡æœ¬ï¼ˆä¹¦ç±ã€ç½‘é¡µã€ä»£ç ç­‰ï¼‰    â”‚    â”‚
â”‚  â”‚  ç›®æ ‡ï¼šå­¦ä¹ è¯­è¨€çš„é€šç”¨æ¨¡å¼å’ŒçŸ¥è¯†               â”‚    â”‚
â”‚  â”‚  æ–¹æ³•ï¼šè‡ªç›‘ç£å­¦ä¹ ï¼ˆå¦‚é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼‰           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å¾®è°ƒé˜¶æ®µ (Fine-tuning)             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  è¾“å…¥ï¼šç‰¹å®šä»»åŠ¡çš„æœ‰æ ‡ç­¾æ•°æ®                   â”‚    â”‚
â”‚  â”‚  ç›®æ ‡ï¼šé€‚é…ç‰¹å®šä»»åŠ¡ï¼ˆå¦‚å¯¹è¯ã€é—®ç­”ï¼‰           â”‚    â”‚
â”‚  â”‚  æ–¹æ³•ï¼šç›‘ç£å­¦ä¹ /å¼ºåŒ– â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€å­¦ä¹                      â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. æ¶ŒçŽ°èƒ½åŠ›

å½“æ¨¡åž‹å‚æ•°è§„æ¨¡è¶…è¿‡æŸä¸ªä¸´ç•Œç‚¹æ—¶ï¼Œä¼šå‡ºçŽ°"æ¶ŒçŽ°èƒ½åŠ›"ï¼ˆEmergent Abilitiesï¼‰ï¼Œå³åœ¨å°æ¨¡åž‹ä¸Šä¸å­˜åœ¨çš„èƒ½åŠ›çªç„¶å‡ºçŽ°ï¼š

- **å°‘æ ·æœ¬å­¦ä¹ ï¼ˆFew-shot Learningï¼‰**ï¼šåªéœ€å°‘é‡ç¤ºä¾‹å°±èƒ½å­¦ä¹ æ–°ä»»åŠ¡
- **æ€ç»´é“¾ï¼ˆChain-of-Thoughtï¼‰**ï¼šèƒ½å¤Ÿè¿›è¡Œå¤šæ­¥æŽ¨ç†
- **æŒ‡ä»¤éµå¾ªï¼ˆInstruction Followingï¼‰**ï¼šç†è§£å¹¶æ‰§è¡Œå¤æ‚æŒ‡ä»¤

## LLM çš„å…³é”®æŠ€æœ¯ç»„ä»¶

### 1. Transformer æž¶æž„

Transformer æ˜¯å½“å‰ LLM çš„æ ¸å¿ƒæž¶æž„ï¼Œè¯¦æƒ…è§ [1.3_Transformeræž¶æž„è¯¦è§£](./1.3_Transformeræž¶æž„è¯¦è§£/readme.md)

### 2. æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰

æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡åž‹èƒ½å¤Ÿ"å…³æ³¨"è¾“å…¥ä¸­æœ€é‡è¦çš„éƒ¨åˆ†ï¼š

```python
# æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒæ€æƒ³ï¼ˆç®€åŒ–ç‰ˆï¼‰
def attention(query, key, value):
    # 1. è®¡ç®—æŸ¥è¯¢å’Œé”®çš„ç›¸ä¼¼åº¦
    scores = dot_product(query, key.transpose())
    
    # 2. å¯¹ç›¸ä¼¼åº¦è¿›è¡Œå½’ä¸€åŒ–ï¼ˆsoftmaxï¼‰
    weights = softmax(scores / sqrt(d_k))
    
    # 3. æ ¹æ®æƒé‡å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œ
    output = dot_product(weights, value)
    return output
```

### 3. è¯åµŒå…¥ï¼ˆWord Embeddingï¼‰

å°†æ–‡å­—è½¬æ¢ä¸ºè®¡ç®—æœºèƒ½ç†è§£çš„å‘é‡è¡¨ç¤ºï¼š

```
æ–‡æœ¬: "ä»Šå¤© å¤©æ°” å¾ˆå¥½"
       â†“
å‘é‡: [0.12, -0.34, 0.56, ...]  [0.78, 0.23, -0.45, ...]  [0.34, 0.67, 0.89, ...]
```

### 4. ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰

ç”±äºŽ Transformer å¹¶è¡Œå¤„ç†æ‰€æœ‰è¯ï¼Œéœ€è¦ç”¨ä½ç½®ç¼–ç æ¥å‘Šè¯‰æ¨¡åž‹è¯çš„é¡ºåºï¼š

```python
# ä½ç½®ç¼–ç çš„ç®€åŒ–ç¤ºä¾‹
def get_position_encoding(position, d_model):
    # å¶æ•°ä½ç½®ä½¿ç”¨æ­£å¼¦å‡½æ•°
    pe[position, 0::2] = sin(position / 10000^(2i/d_model))
    # å¥‡æ•°ä½ç½®ä½¿ç”¨ä½™å¼¦å‡½æ•°
    pe[position, 1::2] = cos(position / 10000^(2i/d_model))
    return pe
```

## LLM çš„è®­ç»ƒè¿‡ç¨‹

### ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒ

```
è®­ç»ƒæ•°æ®æ¥æºï¼š
â”œâ”€â”€ äº’è”ç½‘ç½‘é¡µï¼ˆWikipediaã€Common Crawlï¼‰
â”œâ”€â”€ ä¹¦ç±ï¼ˆProject Gutenbergï¼‰
â”œâ”€â”€ ä»£ç ï¼ˆGitHubï¼‰
â”œâ”€â”€ ç»´åŸºç™¾ç§‘
â””â”€â”€ æ–°é—»æ–‡ç« 

è®­ç»ƒç›®æ ‡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆNext Token Predictionï¼‰
æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µæŸå¤±ï¼ˆCross-Entropy Lossï¼‰
```

### ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒ

```
å¾®è°ƒæ–¹æ³•ï¼š
â”œâ”€â”€ ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼šä½¿ç”¨æ ‡æ³¨æ•°æ®è®­ç»ƒ
â”œâ”€â”€ äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ï¼šæ ¹æ®äººç±»åé¦ˆä¼˜åŒ–
â””â”€â”€ æç¤ºå¾®è°ƒï¼ˆPrompt Tuningï¼‰ï¼šè°ƒæ•´æç¤ºæ¨¡æ¿
```

### ç¬¬ä¸‰é˜¶æ®µï¼šå¯¹é½ï¼ˆå¯é€‰ï¼‰

ç¡®ä¿æ¨¡åž‹è¾“å‡ºç¬¦åˆäººç±»ä»·å€¼è§‚ï¼š
- é¿å…æœ‰å®³å†…å®¹
- æé«˜å›žç­”è´¨é‡
- å¢žå¼ºæœ‰ç”¨æ€§

## LLM çš„åº”ç”¨åœºæ™¯

| åº”ç”¨é¢†åŸŸ | å…·ä½“åœºæ™¯ |
|---------|---------|
| **å¯¹è¯ç³»ç»Ÿ** | æ™ºèƒ½å®¢æœã€ä¸ªäººåŠ©æ‰‹ |
| **å†…å®¹åˆ›ä½œ** | å†™ä½œè¾…åŠ©ã€ä»£ç ç”Ÿæˆ |
| **ä¿¡æ¯æ£€ç´¢** | é—®ç­”ç³»ç»Ÿã€çŸ¥è¯†æŸ¥è¯¢ |
| **æ•™è‚²åŸ¹è®­** | ä½œä¸šè¾…å¯¼ã€è¯­è¨€å­¦ä¹  |
| **åŒ»ç–—å¥åº·** | åŒ»å­¦é—®ç­”ã€å¥åº·å’¨è¯¢ |
| **é‡‘èžåˆ†æž** | æŠ¥å‘Šç”Ÿæˆã€æ•°æ®åˆ†æž |

## å¸¸è§çš„ LLM ä»£è¡¨

### é—­æºæ¨¡åž‹
- **GPT-4**ï¼ˆOpenAIï¼‰ï¼šå½“å‰æœ€å¼ºæ¨¡åž‹ä¹‹ä¸€
- **Claude**ï¼ˆAnthropicï¼‰ï¼šæ“…é•¿é•¿æ–‡æœ¬å¤„ç†
- **Gemini**ï¼ˆGoogleï¼‰ï¼šå¤šæ¨¡æ€èƒ½åŠ›å¼ºå¤§

### å¼€æºæ¨¡åž‹
- **LLaMA**ï¼ˆMetaï¼‰ï¼šå¼€æºç¤¾åŒºæœ€å—æ¬¢è¿Ž
- **Qwen**ï¼ˆé˜¿é‡Œå·´å·´ï¼‰ï¼šä¸­æ–‡èƒ½åŠ›ä¼˜ç§€
- **ChatGLM**ï¼ˆæ¸…åŽï¼‰ï¼šä¸­æ–‡å¯¹è¯æ•ˆæžœå¥½
- **Mistral**ï¼ˆæ³•å›½ï¼‰ï¼šæ€§èƒ½ä¼˜ç§€

## åˆå­¦è€…å¸¸è§é—®é¢˜

### Q1: LLM å’Œä¹‹å‰çš„ NLP æ¨¡åž‹æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

| ä¼ ç»ŸNLPæ¨¡åž‹ | å¤§è¯­è¨€æ¨¡åž‹ |
|------------|-----------|
| éœ€è¦å¤§é‡æ ‡æ³¨æ•°æ® | å¯ä»¥é›¶æ ·æœ¬/å°‘æ ·æœ¬å­¦ä¹  |
| æ¯ä¸ªä»»åŠ¡éœ€è¦å•ç‹¬è®­ç»ƒ | ä¸€ä¸ªæ¨¡åž‹å®Œæˆå¤šç§ä»»åŠ¡ |
| å‚æ•°è¾ƒå°‘ | å‚æ•°è§„æ¨¡å·¨å¤§ |
| èƒ½åŠ›æœ‰é™ | æ¶ŒçŽ°å‡ºæ–°èƒ½åŠ› |

### Q2: ä¸ºä»€ä¹ˆ LLM è¿™ä¹ˆæ¶ˆè€—èµ„æºï¼Ÿ

LLM éœ€è¦å¤§é‡çš„è®¡ç®—èµ„æºæ¥ï¼š
1. **è®­ç»ƒé˜¶æ®µ**ï¼šéœ€è¦æ•°åƒå—GPUè®­ç»ƒæ•°æœˆ
2. **æŽ¨ç†é˜¶æ®µ**ï¼šæ¯æ¬¡å›žç­”éƒ½éœ€è¦å¤§é‡çš„çŸ©é˜µè¿ç®—

### Q3: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ LLMï¼Ÿ

è€ƒè™‘å› ç´ ï¼š
- **ä»»åŠ¡éœ€æ±‚**ï¼šå¯¹è¯ã€å†™ä½œã€ä»£ç ç­‰
- **è¯­è¨€**ï¼šä¸­æ–‡/è‹±æ–‡
- **éƒ¨ç½²çŽ¯å¢ƒ**ï¼šäº‘ç«¯/æœ¬åœ°
- **æˆæœ¬**ï¼šAPIè°ƒç”¨/å¼€æºè‡ªå»º

## æ€»ç»“

æœ¬èŠ‚æˆ‘ä»¬ä»‹ç»äº†å¤§è¯­è¨€æ¨¡åž‹çš„åŸºç¡€æ¦‚å¿µï¼š

1. **LLM æ˜¯ç»è¿‡å¤§è§„æ¨¡æ–‡æœ¬è®­ç»ƒçš„è¯­è¨€æ¨¡åž‹**
2. **å‚æ•°è§„æ¨¡æ˜¯ LLM çš„é‡è¦ç‰¹å¾**
3. **é¢„è®­ç»ƒ + å¾®è°ƒæ˜¯ä¸»æµè®­ç»ƒèŒƒå¼**
4. **Transformer æ˜¯æ ¸å¿ƒæŠ€æœ¯æž¶æž„**
5. **å…·æœ‰æ¶ŒçŽ°èƒ½åŠ›å’Œå¹¿æ³›çš„åº”ç”¨åœºæ™¯**

---

> ðŸ“š ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹  [Transformer æž¶æž„è¯¦è§£](./1.3_Transformeræž¶æž„è¯¦è§£/readme.md)
