# ç¬¬äºŒç« ï¼šå¤§æ¨¡å‹å‘å±•å†ç¨‹

> æœ¬ç« å°†å¸¦ä½ äº†è§£å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•å†å²ï¼Œä»æ—©æœŸæ¢ç´¢åˆ° ChatGPT æ—¶ä»£çš„æ¼”è¿›è¿‡ç¨‹ã€‚

## 2.1 æ—©æœŸæ¢ç´¢ï¼ˆ2013-2017ï¼‰

### 2.1.1 Word2Vec ä¸è¯åµŒå…¥æ—¶ä»£

2013å¹´ï¼ŒGoogle å‘å¸ƒäº† **Word2Vec**ï¼Œè¿™æ˜¯ä¸€ç§å°†å•è¯è½¬æ¢ä¸ºå¯†é›†å‘é‡è¡¨ç¤ºçš„æ–¹æ³•ã€‚è¿™é¡¹å·¥ä½œå¥ å®šäº†ç°ä»£ NLP çš„åŸºç¡€ã€‚

```python
# ä½¿ç”¨ Gensim è®­ç»ƒ Word2Vec
from gensim.models import Word2Vec

# å‡†å¤‡è®­ç»ƒè¯­æ–™
sentences = [
    ['king', 'queen', 'man', 'woman'],
    ['apple', 'fruit', 'orange'],
    ['computer', 'keyboard', 'mouse']
]

# è®­ç»ƒæ¨¡å‹
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1)

# è·å–è¯å‘é‡
king_vector = model.wv['king']
print(f"King å‘é‡ç»´åº¦: {king_vector.shape}")

# è¯ç±»æ¯”ï¼šking - man + woman â‰ˆ queen
similar = model.wv.most_similar(positive=['king', 'woman'], negative=['man'])
print(f"ç±»æ¯”ç»“æœ: {similar}")
```

### 2.1.2 RNN ä¸ LSTM

**å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰** å’Œ **é•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰** æ˜¯æ—©æœŸå¤„ç†åºåˆ—æ•°æ®çš„ä¸»è¦æ–¹æ³•ã€‚

```python
# ç®€å•çš„ LSTM å±‚ç¤ºä¾‹
import torch
import torch.nn as nn

class SimpleLSTM(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # x: (batch_size, seq_len)
        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)
        lstm_out, _ = self.lstm(embedded)  # (batch_size, seq_len, hidden_dim)
        output = self.fc(lstm_out)  # (batch_size, seq_len, vocab_size)
        return output
```

> âš ï¸ **å±€é™**ï¼šRNN å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸é—®é¢˜ï¼Œéš¾ä»¥å¤„ç†é•¿åºåˆ—ã€‚

---

## 2.2 Transformer é©å‘½ï¼ˆ2017-2019ï¼‰

### 2.2.1 Attention Is All You Need (2017)

2017å¹´ï¼ŒGoogle å‘å¸ƒäº†é‡Œç¨‹ç¢‘è®ºæ–‡ **"Attention Is All You Need"**ï¼Œæå‡ºäº† **Transformer** æ¶æ„ã€‚

<div align="center">
<img src="./imgs/transformer_architecture.png" width="600" />
</div>

Transformer çš„æ ¸å¿ƒåˆ›æ–°ï¼š

1. **è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆSelf-Attentionï¼‰**ï¼šç›´æ¥å»ºæ¨¡åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®çš„å…³ç³»
2. **ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰**ï¼šä¿ç•™åºåˆ—é¡ºåºä¿¡æ¯
3. **å¹¶è¡Œè®¡ç®—**ï¼šç›¸æ¯” RNNï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªåºåˆ—

```python
# ä½ç½®ç¼–ç å®ç°
import torch
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        x = x + self.pe[:, :x.size(1), :]
        return x
```

### 2.2.2 BERT çš„è¯ç”Ÿ (2018)

2018å¹´ï¼ŒGoogle å‘å¸ƒ **BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰**ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒå‘ç¼–ç å™¨ï¼Œåœ¨å¤šé¡¹ NLP ä»»åŠ¡ä¸Šåˆ·æ–°äº†è®°å½•ã€‚

BERT çš„å…³é”®åˆ›æ–°ï¼š
- **åŒå‘ç¼–ç **ï¼šåŒæ—¶è€ƒè™‘å·¦å³ä¸Šä¸‹æ–‡
- **é¢„è®­ç»ƒ + å¾®è°ƒ**ï¼šå…ˆåœ¨å¤§é‡æ–‡æœ¬ä¸Šé¢„è®­ç»ƒï¼Œå†å¾®è°ƒç‰¹å®šä»»åŠ¡
- ** MLM + NSP**ï¼šä½¿ç”¨ Masked Language Modeling å’Œ Next Sentence Prediction

```python
# ä½¿ç”¨ HuggingFace BERT è¿›è¡Œæƒ…æ„Ÿåˆ†æ
from transformers import BertTokenizer, BertForSequenceClassification
import torch

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
model = BertForSequenceClassification.from_pretrained(
    'bert-base-chinese', 
    num_labels=2
)

# å‡†å¤‡è¾“å…¥
text = "è¿™ä¸ªäº§å“éå¸¸å¥½ç”¨ï¼"
inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)

# æ¨ç†
model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    prediction = torch.argmax(outputs.logits, dim=-1)
    print(f"æƒ…æ„Ÿé¢„æµ‹: {'æ­£é¢' if prediction.item() == 1 else 'è´Ÿé¢'}")
```

---

## 2.3 GPT æ—¶ä»£ï¼ˆ2018-2020ï¼‰

### 2.3.1 GPT-1 (2018)

**GPTï¼ˆGenerative Pre-trained Transformerï¼‰** æ˜¯ OpenAI æå‡ºçš„ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹ã€‚

| ç‰ˆæœ¬ | å‚æ•° | å‘å¸ƒæ—¶é—´ | ç‰¹ç‚¹ |
|------|------|----------|------|
| GPT-1 | 117M | 2018.06 | é¦–ä¸ªç”Ÿæˆå¼é¢„è®­ç»ƒ Transformer |

GPT-1 çš„æ¶æ„ï¼š
- **å•å‘ï¼ˆä»å·¦åˆ°å³ï¼‰Transformer è§£ç å™¨**
- **æ— ç›‘ç£é¢„è®­ç»ƒ + æœ‰ç›‘ç£å¾®è°ƒ**

### 2.3.2 GPT-2 (2019)

**GPT-2** äº 2019å¹´å‘å¸ƒï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ° **15äº¿ï¼ˆ1.5Bï¼‰**ã€‚

å…³é”®çªç ´ï¼š
- **é›¶æ ·æœ¬å­¦ä¹ ï¼ˆZero-shot Learningï¼‰**ï¼šæ— éœ€å¾®è°ƒï¼Œç›´æ¥åœ¨ prompt ä¸­æŒ‡å®šä»»åŠ¡
- **æ›´å¤§è§„æ¨¡**ï¼š15B å‚æ•°ï¼Œåœ¨ 40GB æ–‡æœ¬ä¸Šè®­ç»ƒ

```python
# GPT-2 é›¶æ ·æœ¬å­¦ä¹ ç¤ºä¾‹
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# åŠ è½½æ¨¡å‹
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# é›¶æ ·æœ¬ä»»åŠ¡ï¼šç¿»è¯‘
prompt = "Translate to French: Hello, how are you?"
inputs = tokenizer.encode(prompt, return_tensors='pt')

# ç”Ÿæˆ
outputs = model.generate(inputs, max_length=50, num_return_sequences=1)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"è¾“å…¥: {prompt}")
print(f"è¾“å‡º: {result}")
```

### 2.3.3 BERT vs GPT å¯¹æ¯”

| ç‰¹æ€§ | BERT | GPT |
|------|------|-----|
| æ¶æ„ | ç¼–ç å™¨ï¼ˆEncoderï¼‰ | è§£ç å™¨ï¼ˆDecoderï¼‰ |
| æ–¹å‘ | åŒå‘ | å•å‘ï¼ˆä»å·¦åˆ°å³ï¼‰ |
| é¢„è®­ç»ƒä»»åŠ¡ | MLM + NSP | è¯­è¨€å»ºæ¨¡ |
| é€‚ç”¨åœºæ™¯ | ç†è§£ä»»åŠ¡ | ç”Ÿæˆä»»åŠ¡ |

---

## 2.4 å¤§æ¨¡å‹çˆ†å‘ï¼ˆ2020-2022ï¼‰

### 2.4.1 GPT-3 (2020)

2020å¹´ï¼ŒOpenAI å‘å¸ƒ **GPT-3**ï¼Œå‚æ•°è§„æ¨¡è¾¾åˆ° **1750äº¿ï¼ˆ175Bï¼‰**ï¼Œè¿™æ˜¯é¦–æ¬¡å‡ºç°"æ¶Œç°èƒ½åŠ›"çš„æ¨¡å‹ã€‚

```python
# ä½¿ç”¨ OpenAI API è°ƒç”¨ GPT-3
import openai

openai.api_key = "your-api-key"

response = openai.Completion.create(
    model="text-davinci-003",
    prompt="Explain quantum computing in simple terms:",
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].text)
```

GPT-3 çš„å…³é”®å‘ç°ï¼š
- **æ¶Œç°èƒ½åŠ›**ï¼šå¤§è§„æ¨¡å‚æ•°å¸¦æ¥äº†å°æ¨¡å‹ä¸å…·å¤‡çš„èƒ½åŠ›
- **ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-context Learningï¼‰**ï¼šé€šè¿‡ prompt ç¤ºä¾‹å­¦ä¹ ä»»åŠ¡
- **æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰**ï¼šæ¨ç†èƒ½åŠ›å¼€å§‹æ˜¾ç°

### 2.4.2 å…¶ä»–é‡è¦æ¨¡å‹ï¼ˆ2020-2022ï¼‰

| æ¨¡å‹ | æœºæ„ | å‚æ•° | ç‰¹ç‚¹ |
|------|------|------|------|
| T5 | Google | 11B | æ–‡æœ¬åˆ°æ–‡æœ¬ç»Ÿä¸€æ¡†æ¶ |
| GPT-J | EleutherAI | 6B | å¼€æº GPT-3 æ›¿ä»£å“ |
| Gopher | DeepMind | 280B | å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ |
| Chinchilla | DeepMind | 70B | å¼ºè°ƒæ•°æ®è´¨é‡ |
| PaLM | Google | 540B | å¤šè¯­è¨€ã€æ¨ç†èƒ½åŠ› |

---

## 2.5 ChatGPT æ—¶ä»£ï¼ˆ2022-è‡³ä»Šï¼‰

### 2.5.1 ChatGPT çš„è¯ç”Ÿ (2022.11)

2022å¹´11æœˆ30æ—¥ï¼ŒOpenAI å‘å¸ƒ **ChatGPT**ï¼ŒåŸºäº GPT-3.5 å¾®è°ƒè€Œæˆã€‚

ChatGPT çš„åˆ›æ–°ï¼š
- **å¯¹è¯ä¼˜åŒ–**ï¼šä¸“é—¨é’ˆå¯¹å¯¹è¯åœºæ™¯è¿›è¡Œå¾®è°ƒ
- **äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰**ï¼šä½¿ç”¨äººç±»åé¦ˆæå‡å¯¹é½èƒ½åŠ›
- **äººäººå¯ç”¨çš„ AI åŠ©æ‰‹**ï¼šé™ä½äº† AI ä½¿ç”¨é—¨æ§›

> ğŸ’¡ **å†å²æ—¶åˆ»**ï¼šChatGPT å‘å¸ƒå 5 å¤©å†…ç”¨æˆ·çªç ´ 100 ä¸‡ï¼Œ2 ä¸ªæœˆå†…çªç ´ 1 äº¿ï¼

### 2.5.2 GPT-4 (2023.03)

2023å¹´3æœˆï¼ŒOpenAI å‘å¸ƒ **GPT-4**ï¼Œè¿™æ˜¯é¦–ä¸ªå¤šæ¨¡æ€å¤§æ¨¡å‹ã€‚

GPT-4 çš„èƒ½åŠ›ï¼š
- âœ… æ›´å¼ºçš„æ¨ç†èƒ½åŠ›
- âœ… å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒç†è§£ï¼‰
- âœ… æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆ128Kï¼‰
- âœ… ä»£ç èƒ½åŠ›æ˜¾è‘—æå‡

```python
# GPT-4 API è°ƒç”¨ç¤ºä¾‹ï¼ˆå¤šæ¨¡æ€ï¼‰
from openai import OpenAI

client = OpenAI(api_key="your-api-key")

response = client.chat.completions.create(
    model="gpt-4-vision-preview",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"},
                {
                    "type": "image_url",
                    "image_url": {"url": "https://example.com/image.jpg"}
                }
            ]
        }
    ],
    max_tokens=300
)

print(response.choices[0].message.content)
```

### 2.5.3 å¼€æºæ¨¡å‹å´›èµ· (2023-2024)

2023-2024å¹´ï¼Œå¼€æºå¤§æ¨¡å‹è“¬å‹ƒå‘å±•ï¼š

| æ¨¡å‹ | æœºæ„ | ç‰¹ç‚¹ |
|------|------|------|
| LLaMA | Meta | å¼€æºã€æ€§èƒ½å¼º |
| LLaMA 2 | Meta | å¯å•†ç”¨ã€70B |
| LLaMA 3 | Meta | æŒ‡ä»¤å¾®è°ƒã€å¤šè¯­è¨€ |
| Mistral | Mistral AI | é«˜æ€§èƒ½ã€Apache è®¸å¯ |
| Qwen | é˜¿é‡Œå·´å·´ | ä¸­æ–‡ä¼˜åŒ–ã€å¼€æº |
| ChatGLM | æ™ºè°±AI | ä¸­æ–‡å¯¹è¯ã€å¼€æº |

```python
# ä½¿ç”¨ transformers åŠ è½½æœ¬åœ° LLaMA æ¨¡å‹
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name, 
    device_map="auto",
    load_in_8bit=True  # 8ä½é‡åŒ–ï¼Œå‡å°‘æ˜¾å­˜
)

# å¯¹è¯ç”Ÿæˆ
prompt = "Explain machine learning in simple terms"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=200)
result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

---

## 2.6 ä¸­å›½å¤§æ¨¡å‹å‘å±•

### 2.6.1 ä¸»è¦ç©å®¶

| æ¨¡å‹ | æœºæ„ | å‘å¸ƒæ—¶é—´ |
|------|------|----------|
| ERNIE Bot | ç™¾åº¦ | 2023.03 |
| é€šä¹‰åƒé—® | é˜¿é‡Œå·´å·´ | 2023.04 |
| è®¯é£æ˜Ÿç« | ç§‘å¤§è®¯é£ | 2023.05 |
| æ™ºè°±æ¸…è¨€ | æ™ºè°±AI | 2023.08 |
| Kimi | æœˆä¹‹æš—é¢ | 2023.10 |
| æ–‡å¿ƒä¸€è¨€ | ç™¾åº¦ | 2023.10 |

### 2.6.2 å‘å±•ç‰¹ç‚¹

1. **ä¸­æ–‡ä¼˜åŒ–**ï¼šé’ˆå¯¹ä¸­æ–‡è¯­æ–™å’Œè¯­è¨€ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–
2. **å¼€æºè´¡çŒ®**ï¼šQwenã€ChatGLM ç­‰å¼€æºæ¨¡å‹
3. **åº”ç”¨è½åœ°**ï¼šå¿«é€Ÿåœ¨å„ä¸ªè¡Œä¸šåœºæ™¯è½åœ°

---

## 2.7 å‘å±•è¶‹åŠ¿ä¸å±•æœ›

### å½“å‰è¶‹åŠ¿

ğŸ“ˆ **æ›´å¤§è§„æ¨¡**ï¼šæ¨¡å‹å‚æ•°ä»åœ¨å¢é•¿
ğŸ“ˆ **å¤šæ¨¡æ€**ï¼šèåˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘
ğŸ“ˆ **é•¿ä¸Šä¸‹æ–‡**ï¼šæ”¯æŒæ›´é•¿çš„è¾“å…¥
ğŸ“ˆ **é«˜æ•ˆæ¨ç†**ï¼šé‡åŒ–ã€è’¸é¦æŠ€æœ¯
ğŸ“ˆ **Agent**ï¼šè‡ªä¸»å®Œæˆä»»åŠ¡

### æœªæ¥å±•æœ›

ğŸ”® **AGI æ¢ç´¢**ï¼šé€šç”¨äººå·¥æ™ºèƒ½çš„ç›®æ ‡
ğŸ”® **å¼€æºç”Ÿæ€**ï¼šæ›´å¤šå¼€æºæ¨¡å‹å’Œåº”ç”¨
ğŸ”® **å‚ç›´é¢†åŸŸ**ï¼šåŒ»ç–—ã€æ³•å¾‹ã€æ•™è‚²ç­‰ä¸“ä¸šæ¨¡å‹
ğŸ”® **äººæœºåä½œ**ï¼šAI è¾…åŠ©åˆ›ä½œå’Œå†³ç­–

---

## 2.8 æœ¬ç« å°ç»“

æœ¬ç« æˆ‘ä»¬å­¦ä¹ äº†å¤§è¯­è¨€æ¨¡å‹çš„å‘å±•å†ç¨‹ï¼š

1. **æ—©æœŸæ¢ç´¢ï¼ˆ2013-2017ï¼‰**ï¼šWord2Vecã€RNNã€LSTM
2. **Transformer é©å‘½ï¼ˆ2017-2019ï¼‰**ï¼šAttention æœºåˆ¶ã€BERT
3. **GPT æ—¶ä»£ï¼ˆ2018-2020ï¼‰**ï¼šGPT-1/2/3ï¼Œé›¶æ ·æœ¬å­¦ä¹ 
4. **å¤§æ¨¡å‹çˆ†å‘ï¼ˆ2020-2022ï¼‰**ï¼šæ¶Œç°èƒ½åŠ›ã€å¤šæ¨¡å‹ç«äº‰
5. **ChatGPT æ—¶ä»£ï¼ˆ2022-è‡³ä»Šï¼‰**ï¼šå¯¹è¯ AIã€å¤šæ¨¡æ€ã€å¼€æºå´›èµ·

---

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165)
- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)

---

*ä¸‹ä¸€ç« æˆ‘ä»¬å°†å­¦ä¹  Transformer çš„æŠ€æœ¯åŸç†ï¼Œæ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„è¿ä½œæ–¹å¼ã€‚*
